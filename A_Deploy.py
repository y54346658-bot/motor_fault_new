# åœ¨å®Œæˆæ‰€æœ‰çš„åŠŸèƒ½çš„é¡µé¢åŸºç¡€ä¸Šï¼Œå¼•å…¥DeepSeekï¼Œä¼˜å…ˆè°ƒç”¨DeepSeekå¯¹ç”¨æˆ·é—®é¢˜è¿›è¡Œå›ç­”ã€‚
# æ³¨æ„ï¼šKIMI APIå…è´¹ï¼ŒDeepSeek APIæ”¶è´¹ï¼Œéœ€è¦ç”³è¯·API Keyã€‚

# å·²æˆåŠŸé€šè¿‡RAGçš„æ–¹å¼åŠ è½½æ•…éšœæ‰‹å†Œï¼Œå¹¶ä½¿ç”¨CNN-1Dæ¨¡å‹è¿›è¡Œæ•…éšœè¯Šæ–­ã€‚

# ä½¿ç”¨æ›´è½»é‡çº§çš„æ¨¡å‹ all-MiniLM-L6-v2

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¿…è¦çš„å¯¼å…¥
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from sklearn.preprocessing import normalize as sklearn_normalize
import streamlit as st
import torch
import torch.nn as nn
import pandas as pd
import matplotlib.pyplot as plt
import os
import requests
import time
import re
import io
from datetime import datetime
from collections import Counter
import PyPDF2
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
from reportlab.platypus import ListFlowable, ListItem, Paragraph
from reportlab.lib.styles import ListStyle
import zipfile
import tempfile
from pathlib import Path
import shutil
import huggingface_hub
from huggingface_hub import hf_hub_download
from matplotlib import font_manager as fm
import matplotlib as mpl
import platform
import tempfile
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.units import inch
import os
import tempfile
import base64
import requests

# è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ—¶åŒºï¼Œè¿™é‡Œä½¿ç”¨UTC+8åŒ—äº¬æ—¶é—´ä¸ºä¾‹ï¼‰
current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜çš„å‡½æ•°
def setup_chinese_font():
    """
    è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒï¼Œé€‚ç”¨äºStreamlit Cloudç¯å¢ƒ
    ä¼˜å…ˆä½¿ç”¨é¡¹ç›®ä¸­çš„å­—ä½“æ–‡ä»¶ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å°è¯•ç³»ç»Ÿå­—ä½“
    """
    # æ–¹æ³•1: å°è¯•ä½¿ç”¨é¡¹ç›®ä¸­çš„å­—ä½“æ–‡ä»¶
    font_paths = [
        "./fonts/simhei.ttf"      # é»‘ä½“
    ]
    
    for font_path in font_paths:
        if os.path.exists(font_path):
            try:
                # æ³¨å†Œå­—ä½“
                fm.fontManager.addfont(font_path)
                font_name = fm.FontProperties(fname=font_path).get_name()
                plt.rcParams['font.family'] = [font_name, 'sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
                st.success(f"æˆåŠŸåŠ è½½å­—ä½“: {font_name}")
                return True
            except Exception as e:
                st.warning(f"å­—ä½“åŠ è½½å¤±è´¥ {font_path}: {e}")
    
    # æ–¹æ³•2: å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“ (é€‚ç”¨äºStreamlit Cloudçš„Linuxç¯å¢ƒ)
    system_fonts = [
        "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
        "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf"
    ]
    
    for font_path in system_fonts:
        if os.path.exists(font_path):
            try:
                # æ³¨å†Œå­—ä½“
                fm.fontManager.addfont(font_path)
                font_name = fm.FontProperties(fname=font_path).get_name()
                plt.rcParams['font.family'] = [font_name, 'sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
                st.success(f"æˆåŠŸåŠ è½½ç³»ç»Ÿå­—ä½“: {font_name}")
                return True
            except Exception as e:
                st.warning(f"ç³»ç»Ÿå­—ä½“åŠ è½½å¤±è´¥ {font_path}: {e}")
    
    # æ–¹æ³•3: å°è¯•ä½¿ç”¨matplotlibå†…ç½®çš„å­—ä½“åˆ«å
    try:
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False
        st.info("ä½¿ç”¨é»˜è®¤å­—ä½“åˆ«å")
        return True
    except Exception as e:
        st.error(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        return False

# è®¾ç½®ä¸­æ–‡å­—ä½“
if setup_chinese_font():
    st.success("ä¸­æ–‡å­—ä½“é…ç½®æˆåŠŸ")
else:
    st.error("ä¸­æ–‡å­—ä½“é…ç½®å¤±è´¥ï¼Œä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†")

# å®šä¹‰CNN-1Dæ¨¡å‹
class CNN1DModel(nn.Module):
    def __init__(self, conv_archs, num_classes, batch_size, input_channels=1):
        super(CNN1DModel, self).__init__()
        self.batch_size = batch_size
        self.conv_arch = conv_archs
        self.input_channels = input_channels
        self.features = self._make_layers()
        self.avgpool = nn.AdaptiveAvgPool1d(9)
        self.classifier = nn.Sequential(
            nn.Linear(128 * 3 * 3, 500),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(500, num_classes),
        )
    
    def _make_layers(self):
        layers = []
        for (num_convs, out_channels) in self.conv_arch:
            for _ in range(num_convs):
                layers.append(nn.Conv1d(self.input_channels, out_channels, kernel_size=3, padding=1))
                layers.append(nn.ReLU(inplace=True))
                self.input_channels = out_channels
            layers.append(nn.MaxPool1d(kernel_size=2, stride=2))
        return nn.Sequential(*layers)

    def forward(self, input_seq):
        batch_size = input_seq.size(0)
        input_seq = input_seq.view(batch_size, 1, 1024)
        features = self.features(input_seq)
        x = self.avgpool(features)
        flat_tensor = x.view(batch_size, -1)
        output = self.classifier(flat_tensor)
        return output

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="ç”µæœºæ•…éšœè¯Šæ–­ç³»ç»Ÿ", page_icon="ğŸ”§", layout="wide")

# åº”ç”¨æ ‡é¢˜
st.title("ğŸ”§ ç”µæœºæ•…éšœè¯Šæ–­ç³»ç»Ÿ")

# è®¾å¤‡è®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å®šä¹‰æ•…éšœç±»å‹æ˜ å°„
fault_type_mapping = {
    0: "æ­£å¸¸çŠ¶æ€ (de_normal)",
    1: "7milå†…åœˆæ•…éšœ (de_7_inner)",
    2: "7milæ»šåŠ¨ä½“æ•…éšœ (de_7_ball)",
    3: "7milå¤–åœˆæ•…éšœ (de_7_outer)",
    4: "14milå†…åœˆæ•…éšœ (de_14_inner)",
    5: "14milæ»šåŠ¨ä½“æ•…éšœ (de_14_ball)",
    6: "14milå¤–åœˆæ•…éšœ (de_14_outer)",
    7: "21milå†…åœˆæ•…éšœ (de_21_inner)",
    8: "21milæ»šåŠ¨ä½“æ•…éšœ (de_21_ball)",
    9: "21milå¤–åœˆæ•…éšœ (de_21_outer)"
}

# ä¿®æ”¹ load_model å‡½æ•°
@st.cache_resource
def load_model():
    try:
        if not os.path.exists('best_model_cnn1d.pt'):
            st.error("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return None
        
        # æ·»åŠ  weights_only=False å‚æ•°
        model = torch.load('best_model_cnn1d.pt', map_location=device, weights_only=False)
        model.eval()
        return model
    except Exception as ex:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {ex}")
        return None

# ä¸‹è½½å¹¶åŠ è½½ all-MiniLM-L6-v2 åµŒå…¥æ¨¡å‹
# ç¡®ä¿ç›®å½•å­˜åœ¨
def ensure_directory_exists(path):
    """ç¡®ä¿æŒ‡å®šè·¯å¾„çš„ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    os.makedirs(path, exist_ok=True)
    return path

# ä¸‹è½½å¹¶åŠ è½½ all-MiniLM-L6-v2 åµŒå…¥æ¨¡å‹
@st.cache_resource(show_spinner=False)
def load_embedding_model():
    try:
        # æ¨¡å‹æœ¬åœ°ä¿å­˜è·¯å¾„
        base_model_path = "./local_models_new"
        model_path = os.path.join(base_model_path, "all-MiniLM-L6-v2")
        pooling_dir = os.path.join(model_path, "1_Pooling")
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•éƒ½å­˜åœ¨
        ensure_directory_exists(base_model_path)
        ensure_directory_exists(model_path)
        ensure_directory_exists(pooling_dir)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
        if not os.path.exists(os.path.join(model_path, "pytorch_model.bin")):
            with st.spinner("æ­£åœ¨ä¸‹è½½ all-MiniLM-L6-v2 æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ..."):
                # åŸºç¡€URL
                base_url = "https://hf-mirror.com/sentence-transformers/all-MiniLM-L6-v2/resolve/main"
                
                # éœ€è¦ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
                files_to_download = [
                    "pytorch_model.bin",
                    "config.json",
                    "tokenizer.json",
                    "vocab.txt",
                    "special_tokens_map.json",
                    "tokenizer_config.json",
                    "sentence_bert_config.json",
                    "modules.json",
                    "1_Pooling/config.json"
                ]
                
                # ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
                for filename in files_to_download:
                    # æ„å»ºå®Œæ•´çš„URL
                    file_url = f"{base_url}/{filename}"
                    
                    # æ„å»ºæœ¬åœ°ä¿å­˜è·¯å¾„
                    if "/" in filename:
                        # å¤„ç†åµŒå¥—ç›®å½•ç»“æ„
                        dir_name = os.path.dirname(filename)
                        file_dir = os.path.join(model_path, dir_name)
                        ensure_directory_exists(file_dir)
                        filepath = os.path.join(model_path, filename)
                    else:
                        filepath = os.path.join(model_path, filename)
                    
                    # ä¸‹è½½æ–‡ä»¶
                    response = requests.get(file_url, stream=True)
                    if response.status_code == 200:
                        with open(filepath, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                    else:
                        st.error(f"ä¸‹è½½ {filename} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        return None
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
        required_files = [
            "pytorch_model.bin",
            "config.json",
            "tokenizer.json",
            "vocab.txt",
            "special_tokens_map.json",
            "tokenizer_config.json",
            "sentence_bert_config.json",
            "modules.json",
            "1_Pooling/config.json"
        ]
        
        missing_files = []
        for file in required_files:
            if not os.path.exists(os.path.join(model_path, file)):
                missing_files.append(file)
        
        if missing_files:
            st.error(f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing_files)}")
            # å°è¯•åˆ é™¤ä¸å®Œæ•´çš„æ¨¡å‹å¹¶é‡æ–°ä¸‹è½½
            shutil.rmtree(model_path, ignore_errors=True)
            return load_embedding_model()  # é€’å½’è°ƒç”¨è‡ªèº«é‡æ–°ä¸‹è½½
        
        # åŠ è½½æœ¬åœ°æ¨¡å‹
        model = SentenceTransformer(model_path)
        st.success("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ! (ä½¿ç”¨ all-MiniLM-L6-v2)")
        return model
    except Exception as e:
        st.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        st.error(traceback.format_exc())
        # å°è¯•åˆ é™¤å¯èƒ½æŸåçš„æ¨¡å‹æ–‡ä»¶
        if 'model_path' in locals():
            shutil.rmtree(model_path, ignore_errors=True)
        return None

# æ•°æ®åˆ†å‰²å‡½æ•°
def split_data_with_overlap(data, time_steps, label, overlap_ratio=0.5):
    stride = int(time_steps * (1 - overlap_ratio))
    samples = (len(data) - time_steps) // stride + 1
    data_list = []
    for i in range(samples):
        start_idx = i * stride
        end_idx = start_idx + time_steps
        temp_data = data[start_idx:end_idx].tolist()
        temp_data.append(label)
        data_list.append(temp_data)
    columns = list(range(time_steps)) + ['label']
    result_df = pd.DataFrame(data_list, columns=columns)
    return result_df

# å½’ä¸€åŒ–å‡½æ•° (é‡å‘½åä»¥é¿å…ä¸sklearnå†²çª)
def normalize_data(data):
    return (data - np.min(data)) / (np.max(data) - np.min(data))

# ==============================
# ä¼˜åŒ–åçš„å‘é‡çŸ¥è¯†åº“ç›¸å…³å‡½æ•°
# ==============================
def create_vector_knowledge_base_optimized(text, embedding_model, chunk_size=500, overlap=50, batch_size=16):
    """
    ä¼˜åŒ–ç‰ˆçš„å‘é‡çŸ¥è¯†åº“åˆ›å»ºå‡½æ•°
    :param text: æ•…éšœæ‰‹å†Œæ–‡æœ¬
    :param embedding_model: åµŒå…¥æ¨¡å‹
    :param chunk_size: æ–‡æœ¬å—å¤§å°
    :param overlap: å—é—´é‡å å¤§å°
    :param batch_size: æ‰¹é‡å¤„ç†å¤§å°
    :return: FAISSç´¢å¼•å’Œæ–‡æœ¬å—åˆ—è¡¨
    """
    # æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§
    if overlap >= chunk_size:
        st.error("é”™è¯¯: é‡å å¤§å°å¿…é¡»å°äºæ–‡æœ¬å—å¤§å°")
        return None, []
    
    if len(text) == 0:
        st.warning("æ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºçŸ¥è¯†åº“")
        return None, []
    
    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœè¿‡é•¿åˆ™æé†’ç”¨æˆ·
    if len(text) > 1000000:  # çº¦100ä¸‡å­—ç¬¦
        st.warning(f"æ–‡æœ¬é•¿åº¦è¾ƒå¤§ ({len(text)} å­—ç¬¦)ï¼Œå¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ•…éšœæ‰‹å†Œæˆ–å¢åŠ å—å¤§å°ã€‚")
    
    # åˆ†å‰²æ–‡æœ¬ä¸ºå—
    chunks = []
    start = 0
    
    # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºå—å¤§å°ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
    if len(text) <= chunk_size:
        chunks.append(text)
    else:
        # æ­£å¸¸åˆ†å—å¤„ç†
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk = text[start:end]
            chunks.append(chunk)
            
            # å¦‚æœå·²ç»åˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œåˆ™é€€å‡ºå¾ªç¯
            if end == len(text):
                break
                
            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = end - overlap
    
    st.info(f"å·²å°†æ–‡æœ¬åˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
    
    # æ˜¾ç¤ºè¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # æ£€æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    if embedding_model is None:
        st.warning("åµŒå…¥æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬æœç´¢æ¨¡å¼")
        progress_bar.progress(1.0)
        status_text.text("å®Œæˆ!")
        return None, chunks
    
    try:
        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ˜¾è‘—æé«˜æ€§èƒ½ï¼‰
        embeddings = []
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        # æ·»åŠ ä¸­æ–­æŒ‰é’®
        stop_processing = False
        if total_batches > 1:  # åªæœ‰åœ¨éœ€è¦å¤„ç†å¤šä¸ªæ‰¹æ¬¡æ—¶æ‰æ˜¾ç¤ºåœæ­¢æŒ‰é’®
            if st.button("åœæ­¢å¤„ç†", key="stop_processing_btn"):
                stop_processing = True
        
        processed_count = 0
        
        for i in range(0, len(chunks), batch_size):
            if stop_processing:
                st.warning("ç”¨æˆ·ä¸­æ–­äº†å¤„ç†è¿‡ç¨‹")
                return None, chunks[:i]  # è¿”å›å·²å¤„ç†çš„éƒ¨åˆ†
            
            batch = chunks[i:i+batch_size]
            
            # æ›´æ–°è¿›åº¦
            processed_count += len(batch)
            progress = min(processed_count / len(chunks), 1.0)
            progress_bar.progress(progress)
            status_text.text(f"å¤„ç†ä¸­: {processed_count}/{len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # æ‰¹é‡ç¼–ç  - æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
            try:
                batch_embeddings = embedding_model.encode(
                    batch, 
                    convert_to_tensor=False,
                    show_progress_bar=False,
                    batch_size=min(batch_size, len(batch)),
                    normalize_embeddings=True  # ç›´æ¥å½’ä¸€åŒ–ï¼Œé¿å…åç»­æ­¥éª¤
                )
                embeddings.extend(batch_embeddings)
            except Exception as e:
                st.error(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches} æ—¶å‡ºé”™: {e}")
                # å°è¯•å¤„ç†è¾ƒå°çš„æ‰¹æ¬¡
                if len(batch) > 1:
                    st.info("å°è¯•å‡å°æ‰¹é‡å¤§å°å¤„ç†...")
                    try:
                        # é€ä¸ªå¤„ç†
                        for single_chunk in batch:
                            single_embedding = embedding_model.encode(
                                [single_chunk],
                                convert_to_tensor=False,
                                show_progress_bar=False,
                                normalize_embeddings=True
                            )
                            embeddings.extend(single_embedding)
                    except Exception as e2:
                        st.error(f"å•ä¸ªå¤„ç†ä¹Ÿå¤±è´¥: {e2}")
                        # è·³è¿‡æœ‰é—®é¢˜çš„æ‰¹æ¬¡ç»§ç»­å¤„ç†
                        continue
                else:
                    # è·³è¿‡æœ‰é—®é¢˜çš„æ‰¹æ¬¡ç»§ç»­å¤„ç†
                    continue
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ä»»ä½•åµŒå…¥å‘é‡
        if not embeddings:
            st.error("æœªèƒ½ç”Ÿæˆä»»ä½•åµŒå…¥å‘é‡ï¼Œæ‰€æœ‰å¤„ç†æ‰¹æ¬¡éƒ½å¤±è´¥äº†")
            return None, chunks
            
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings = np.array(embeddings)
        
        # ç¡®ä¿åµŒå…¥å‘é‡æ•°é‡ä¸æ–‡æœ¬å—æ•°é‡ä¸€è‡´
        if len(embeddings) != len(chunks):
            st.warning(f"åµŒå…¥å‘é‡æ•°é‡ ({len(embeddings)}) ä¸æ–‡æœ¬å—æ•°é‡ ({len(chunks)}) ä¸åŒ¹é…")
            # æˆªå–æˆ–å¡«å……ä»¥åŒ¹é…æ•°é‡
            min_len = min(len(embeddings), len(chunks))
            embeddings = embeddings[:min_len]
            chunks = chunks[:min_len]
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        try:
            index.add(embeddings.astype('float32'))
        except Exception as e:
            st.error(f"åˆ›å»ºFAISSç´¢å¼•å¤±è´¥: {e}")
            return None, chunks
        
        progress_bar.progress(1.0)
        status_text.text("å®Œæˆ!")
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        if st.checkbox("æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ", key="show_memory_usage"):
            index_size = index.ntotal * index.d * 4  # å¤§è‡´è®¡ç®—ç´¢å¼•å¤§å°(å­—èŠ‚)
            st.write(f"FAISSç´¢å¼•å¤§å°: {index_size / (1024*1024):.2f} MB")
            st.write(f"æ–‡æœ¬å—æ•°é‡: {len(chunks)}")
            st.write(f"å‘é‡ç»´åº¦: {dimension}")
        
        return index, chunks
    except Exception as e:
        st.error(f"åˆ›å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        progress_bar.progress(1.0)
        status_text.text("å‡ºé”™!")
        return None, chunks

def preprocess_text(text):
    """
    é¢„å¤„ç†æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    :param text: åŸå§‹æ–‡æœ¬
    :return: å¤„ç†åçš„æ–‡æœ¬
    """
    if not text:
        return ""
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹ï¼‰
    text = re.sub(r'[^\w\u4e00-\u9fff\s.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', '', text)
    return text.strip()

def search_vector_knowledge_base(query, index, chunks, embedding_model, top_k=3):
    """
    åœ¨å‘é‡çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£
    :param query: æŸ¥è¯¢æ–‡æœ¬
    :param index: FAISSç´¢å¼•
    :param chunks: æ–‡æœ¬å—åˆ—è¡¨
    :param embedding_model: åµŒå…¥æ¨¡å‹
    :param top_k: è¿”å›æœ€ç›¸å…³çš„kä¸ªç»“æœ
    :return: ç›¸å…³æ–‡æœ¬å—åˆ—è¡¨
    """
    # å¦‚æœç´¢å¼•ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬æœç´¢
    if index is None:
        return search_fallback_knowledge_base(query, chunks, top_k)
    
    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = sklearn_normalize(query_embedding, norm='l2', axis=1)
        
        # æœç´¢ç›¸ä¼¼å‘é‡
        distances, indices = index.search(query_embedding.astype('float32'), top_k)
        
        # è·å–ç›¸å…³æ–‡æœ¬å—
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(chunks):
                results.append({
                    "text": chunks[idx],
                    "score": distances[0][i]
                })
        
        return results
    except Exception as e:
        st.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
        return search_fallback_knowledge_base(query, chunks, top_k)

def search_fallback_knowledge_base(query, chunks, top_k=3):
    """ç®€å•å…³é”®è¯åŒ¹é…æœç´¢"""
    results = []
    query_words = query.lower().split()
    
    for i, chunk in enumerate(chunks):
        score = 0
        chunk_lower = chunk.lower()
        for word in query_words:
            if word in chunk_lower:
                score += 1
        
        if score > 0:
            results.append({
                "text": chunk,
                "score": score / len(query_words)
            })
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰kä¸ª
    results.sort(key=lambda x: x["score"], reverse=True)
    return results[:top_k]

# ==============================
# API ç›¸å…³å‡½æ•°
# ==============================
KIMI_API_URL = "https://api.moonshot.cn/v1/chat/completions"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# è¯»å–é…ç½®
def get_api_config(api_name):
    config = {}
    try:
        if api_name == "kimi":
            config["API_KEY"] = st.secrets["KIMI_API_KEY"]
        elif api_name == "deepseek":
            config["API_KEY"] = st.secrets["DEEPSEEK_API_KEY"]
    except (KeyError, FileNotFoundError):
        if api_name == "kimi":
            config["API_KEY"] = os.environ.get("KIMI_API_KEY", "")
        elif api_name == "deepseek":
            config["API_KEY"] = os.environ.get("DEEPSEEK_API_KEY", "")
    
    if not config["API_KEY"]:
        if api_name == "kimi":
            st.error("æœªæ‰¾åˆ°KIMI APIå¯†é’¥ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–secrets.tomlä¸­è®¾ç½®KIMI_API_KEY")
        elif api_name == "deepseek":
            st.error("æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–secrets.tomlä¸­è®¾ç½®DEEPSEEK_API_KEY")
    
    return config

# ä»æ–‡ä»¶æå–æ–‡æœ¬å†…å®¹
def extract_text_from_file(file):
    """ä»ä¸Šä¼ çš„æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    file_type = file.type
    
    try:
        if file_type == "text/plain":
            # TXTæ–‡ä»¶
            return str(file.read(), "utf-8")
        
        elif file_type == "application/pdf":
            # PDFæ–‡ä»¶
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text
        
        elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            # DOCXæ–‡ä»¶
            doc = Document(file)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        
        else:
            st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            return None
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return None

def call_deepseek_api(messages, max_tokens=2000, temperature=0.7):
    """è°ƒç”¨DeepSeek API"""
    config = get_api_config("deepseek")
    
    # ç¡®ä¿ config ä¸ä¸º None ä¸”åŒ…å« API_KEY
    if not config or not config.get("API_KEY"):
        st.error("æœªæ‰¾åˆ°DeepSeek APIé…ç½®æˆ–APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return None
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {config['API_KEY']}"
    }
    
    # å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
    system_prompt_content = prepare_system_prompt()
    
    system_prompt = {
        "role": "system", 
        "content": system_prompt_content
    }
    
    # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
    api_messages = [system_prompt]
    
    # æ·»åŠ æ‰€æœ‰éç³»ç»Ÿæ¶ˆæ¯
    for msg in messages:
        if isinstance(msg, dict) and msg.get("role") != "system":
            api_messages.append({
                "role": msg.get("role", "user"),
                "content": msg.get("content", "")
            })
    
    data = {
        "model": "deepseek-chat",
        "messages": api_messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }
    
    try:
        with st.spinner("æ­£åœ¨è°ƒç”¨DeepSeek APIï¼Œè¯·ç¨å€™..."):
            response = requests.post(
                DEEPSEEK_API_URL,
                headers=headers,
                json=data,
                timeout=60
            )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            try:
                result = response.json()
                # æ£€æŸ¥å“åº”ç»“æ„
                if (result and isinstance(result, dict) and 
                    "choices" in result and isinstance(result["choices"], list) and 
                    len(result["choices"]) > 0 and 
                    "message" in result["choices"][0] and 
                    "content" in result["choices"][0]["message"]):
                    return result["choices"][0]["message"]["content"]
                else:
                    st.error(f"DeepSeek APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                    return None
            except ValueError:
                st.error("DeepSeek APIè¿”å›äº†æ— æ•ˆçš„JSONå“åº”")
                return None
        elif response.status_code == 401:
            st.error("DeepSeek APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥APIå¯†é’¥")
            return None
        elif response.status_code == 429:
            st.error("DeepSeek APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•")
            return None
        else:
            st.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        st.error("DeepSeek APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•")
        return None
    except requests.exceptions.ConnectionError:
        st.error("ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
        return None
    except Exception as err:
        st.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {str(err)}")
        return None

def call_kimi_api(messages, max_tokens=2000, temperature=0.7):
    """è°ƒç”¨KIMI API"""
    config = get_api_config("kimi")
    
    # ç¡®ä¿ config ä¸ä¸º None ä¸”åŒ…å« API_KEY
    if not config or not config.get("API_KEY"):
        st.error("æœªæ‰¾åˆ°KIMI APIé…ç½®æˆ–APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return None
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {config['API_KEY']}"
    }
    
    # å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
    system_prompt_content = prepare_system_prompt()
    
    system_prompt = {
        "role": "system", 
        "content": system_prompt_content
    }
    
    # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
    api_messages = [system_prompt]
    
    # æ·»åŠ æ‰€æœ‰éç³»ç»Ÿæ¶ˆæ¯
    for msg in messages:
        if isinstance(msg, dict) and msg.get("role") != "system":
            api_messages.append({
                "role": msg.get("role", "user"),
                "content": msg.get("content", "")
            })
    
    data = {
        "model": "moonshot-v1-8k",
        "messages": api_messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": 0.9
    }
    
    try:
        with st.spinner("æ­£åœ¨è°ƒç”¨KIMI APIï¼Œè¯·ç¨å€™..."):
            response = requests.post(
                KIMI_API_URL,
                headers=headers,
                json=data,
                timeout=60
            )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            try:
                result = response.json()
                # æ£€æŸ¥å“åº”ç»“æ„
                if (result and isinstance(result, dict) and 
                    "choices" in result and isinstance(result["choices"], list) and 
                    len(result["choices"]) > 0 and 
                    "message" in result["choices"][0] and 
                    "content" in result["choices"][0]["message"]):
                    return result["choices"][0]["message"]["content"]
                else:
                    st.error(f"KIMI APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                    return None
            except ValueError:
                st.error("KIMI APIè¿”å›äº†æ— æ•ˆçš„JSONå“åº”")
                return None
        elif response.status_code == 401:
            st.error("KIMI APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥APIå¯†é’¥")
            return None
        elif response.status_code == 429:
            st.error("KIMI APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•")
            return None
        else:
            st.error(f"KIMI APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        st.error("KIMI APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•")
        return None
    except requests.exceptions.ConnectionError:
        st.error("ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
        return None
    except Exception as err:
        st.error(f"KIMI APIè¯·æ±‚å¤±è´¥: {str(err)}")
        return None

def prepare_system_prompt():
    """å‡†å¤‡ç³»ç»Ÿæç¤ºè¯"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è¯Šæ–­ç»“æœ
    diagnosis_info = ""
    if 'diagnosis_results' in st.session_state and st.session_state['diagnosis_results'] is not None:
        diagnosis = st.session_state['diagnosis_results']
        # ç¡®ä¿ diagnosis æ˜¯å­—å…¸ç±»å‹
        if isinstance(diagnosis, dict):
            diagnosis_info = f"""
            å·²çŸ¥æŒ¯åŠ¨ä¿¡å·ç»è¿‡CNNæ¨¡å‹è¯Šæ–­ï¼Œç»“æœå¦‚ä¸‹ï¼š
            - è¯Šæ–­ç»“æœ: {diagnosis.get('diagnosis_class', 'æœªçŸ¥')}
            - ç½®ä¿¡åº¦: {diagnosis.get('confidence_level', 0)*100:.2f}%
            - ä¿¡å·ç»Ÿè®¡: {diagnosis.get('analysis_results', 'æ— ')}
            """

            # å¦‚æœæœ‰æŠ¥å‘Šå†…å®¹ï¼Œä¹Ÿæ·»åŠ åˆ°ä¿¡æ¯ä¸­
            if diagnosis.get('report'):
                diagnosis_info += f"\n- è¯¦ç»†æŠ¥å‘Š: {diagnosis.get('report')[:200]}..."  # åªå–å‰200å­—ç¬¦é¿å…è¿‡é•¿
    
    # å‡†å¤‡æ•…éšœæ‰‹å†Œä¿¡æ¯ - ä½¿ç”¨å‘é‡çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹
    manual_info = ""
    
    # æ·»åŠ æ›´ä¸¥æ ¼çš„æ£€æŸ¥æ¡ä»¶
    vector_index_exists = (
        'vector_index' in st.session_state and 
        st.session_state['vector_index'] is not None and
        hasattr(st.session_state['vector_index'], 'ntotal') and
        st.session_state['vector_index'].ntotal > 0
    )
    
    vector_chunks_exists = (
        'vector_chunks' in st.session_state and 
        st.session_state['vector_chunks'] is not None and
        len(st.session_state['vector_chunks']) > 0
    )
    
    if vector_index_exists and vector_chunks_exists:
        # è·å–ç”¨æˆ·çš„æœ€æ–°æŸ¥è¯¢
        user_query = ""
        for msg in reversed(st.session_state["messages"]):
            if msg.get("role") == "user":
                user_query = msg.get("content", "")
                break
        
        if user_query:
            # ä»å‘é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
            embedding_model = load_embedding_model()  # è·å–åµŒå…¥æ¨¡å‹å®ä¾‹æ¥å®æ—¶è½¬æ¢ç”¨æˆ·æŸ¥è¯¢
            if embedding_model:
                relevant_chunks = search_vector_knowledge_base(
                    user_query, 
                    st.session_state['vector_index'],
                    st.session_state['vector_chunks'],
                    embedding_model,
                    top_k=3
                )
                
                if relevant_chunks:
                    manual_info = "ä»¥ä¸‹æ˜¯æ ¹æ®æ•…éšœæ‰‹å†Œæ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n"
                    for i, chunk in enumerate(relevant_chunks):
                        manual_info += f"\n[ç›¸å…³æ®µè½ {i+1}, ç›¸ä¼¼åº¦: {chunk['score']:.3f}]\n{chunk['text']}\n"
    
    # å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
    system_prompt_content = f"""ä½ æ˜¯ä¸€åèµ„æ·±çš„ç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œç†Ÿæ‚‰æŒ¯åŠ¨ä¿¡å·åˆ†æã€è½´æ‰¿/è½¬å­æ•…éšœæœºç†å’Œç»´ä¿®æ–¹æ³•ã€‚

    å½“å‰ç³»ç»Ÿæ—¶é—´ï¼š{current_time}

    {manual_info}

    {diagnosis_info}

    ä½ çš„ä»»åŠ¡æ˜¯ï¼šæ ¹æ®ç”¨æˆ·çš„é—®é¢˜ã€æ•…éšœæ‰‹å†Œå‚è€ƒå’Œå¯ç”¨çš„è¯Šæ–­ç»“æœï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®çš„è§£ç­”å’Œå»ºè®®ã€‚

    è¯·ä¿æŒä¸“ä¸šã€ä¸¥è°¨ã€å·¥ç¨‹åŒ–çš„è¯­æ°”ï¼Œä¸è¦è™šæ„ä¸å­˜åœ¨çš„æ•°æ®ã€‚

    å›ç­”æ—¶è¯·ä¼˜å…ˆå‚è€ƒæ•…éšœæ‰‹å†Œä¸­çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆè¯Šæ–­ç»“æœç»™å‡ºå»ºè®®ã€‚
    """
    
    return system_prompt_content

def call_ai_api(messages, max_tokens=2000, temperature=0.7):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„APIåå¥½è°ƒç”¨ç›¸åº”çš„API"""
    # è·å–ç”¨æˆ·é€‰æ‹©çš„APIåå¥½
    preferred_api = st.session_state.get('preferred_api', 'deepseek')  # é»˜è®¤ä¸ºdeepseek
    
    if preferred_api == "deepseek":
        # å…ˆå°è¯•è°ƒç”¨DeepSeek API
        reply = call_deepseek_api(messages, max_tokens, temperature)
        
        # å¦‚æœDeepSeekè°ƒç”¨å¤±è´¥ï¼Œå°è¯•è°ƒç”¨KIMI API
        if reply is None:
            st.warning("DeepSeek APIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨KIMI API")
            reply = call_kimi_api(messages, max_tokens, temperature)
    else:
        # å…ˆå°è¯•è°ƒç”¨KIMI API
        reply = call_kimi_api(messages, max_tokens, temperature)
        
        # å¦‚æœKIMIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•è°ƒç”¨DeepSeek API
        if reply is None:
            st.warning("KIMI APIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨DeepSeek API")
            reply = call_deepseek_api(messages, max_tokens, temperature)
    
    return reply


# å¤„ç† Markdown æ ¼å¼çš„æ–‡æœ¬
def process_markdown_for_docx(text):
    """
    å°†Markdownæ ¼å¼æ–‡æœ¬å¤„ç†ä¸ºé€‚åˆDOCXçš„æ ¼å¼
    :param text: Markdownæ ¼å¼çš„æ–‡æœ¬
    :return: å¤„ç†åçš„æ–‡æœ¬
    """
    # é¦–å…ˆè§„èŒƒåŒ–æ¢è¡Œç¬¦
    text = re.sub(r'\r\n', '\n', text)  # ç»Ÿä¸€æ¢è¡Œç¬¦
    text = re.sub(r'\n{3,}', '\n\n', text)  # å¤šä¸ªè¿ç»­æ¢è¡Œç¼©å‡ä¸ºä¸¤ä¸ª
    
    # å¤„ç†æ ‡é¢˜ - ç¡®ä¿æ ‡é¢˜åæœ‰é¢å¤–çš„æ¢è¡Œ
    text = re.sub(r'^# (.*)$', r'æ ‡é¢˜1: \1\n\n', text, flags=re.MULTILINE)
    text = re.sub(r'^## (.*)$', r'æ ‡é¢˜2: \1\n\n', text, flags=re.MULTILINE)
    text = re.sub(r'^### (.*)$', r'æ ‡é¢˜3: \1\n\n', text, flags=re.MULTILINE)
    text = re.sub(r'^#### (.*)$', r'æ ‡é¢˜4: \1\n\n', text, flags=re.MULTILINE)
    text = re.sub(r'^##### (.*)$', r'æ ‡é¢˜5: \1\n\n', text, flags=re.MULTILINE)
    text = re.sub(r'^###### (.*)$', r'æ ‡é¢˜6: \1\n\n', text, flags=re.MULTILINE)
    
    # å¤„ç†åŠ ç²— - ä¿ç•™æ ‡è®°ä¾›åç»­å¤„ç†
    text = re.sub(r'\*\*(.*?)\*\*', r'[BOLD]\1[/BOLD]', text)
    
    # å¤„ç†æ–œä½“ - ä¿ç•™æ ‡è®°ä¾›åç»­å¤„ç†
    text = re.sub(r'\*(.*?)\*', r'[ITALIC]\1[/ITALIC]', text)
    
    # å¤„ç†æ— åºåˆ—è¡¨ - ç¡®ä¿åˆ—è¡¨é¡¹åæœ‰æ¢è¡Œ
    text = re.sub(r'^-\s+(.*)$', r'â€¢ \1\n', text, flags=re.MULTILINE)
    text = re.sub(r'^\*\s+(.*)$', r'â€¢ \1\n', text, flags=re.MULTILINE)
    
    # å¤„ç†æœ‰åºåˆ—è¡¨ - ç¡®ä¿åˆ—è¡¨é¡¹åæœ‰æ¢è¡Œ
    def replace_ordered_list(match):
        return f"{match.group(1)}. {match.group(2)}\n"
    
    text = re.sub(r'^(\d+)\.\s+(.*)$', replace_ordered_list, text, flags=re.MULTILINE)
    
    # å¤„ç†åˆ†å‰²çº¿ - ä½¿ç”¨ç‰¹æ®Šæ ‡è®°å¹¶æ·»åŠ é¢å¤–æ¢è¡Œ
    text = re.sub(r'^---+\s*$', r'[HR]\n\n', text, flags=re.MULTILINE)
    text = re.sub(r'^\*\*\*+\s*$', r'[HR]\n\n', text, flags=re.MULTILINE)
    
    # ç¡®ä¿æ®µè½ä¹‹é—´æœ‰è¶³å¤Ÿçš„é—´è·
    text = re.sub(r'\n\n+', '\n\n', text)
    
    # å¤„ç†æ¢è¡Œ - ä½¿ç”¨ç‰¹æ®Šæ ‡è®°
    text = text.replace('\n', '[NEWLINE]')
    
    return text

# å¤„ç† Markdown æ ¼å¼çš„æ–‡æœ¬
def process_markdown(text):
    """
    å°† Markdown æ ¼å¼è½¬æ¢ä¸º ReportLab å¯è¯†åˆ«çš„æ ¼å¼
    """
    # å¤„ç†æ ‡é¢˜ (æ”¯æŒ 1-6 çº§æ ‡é¢˜)
    text = re.sub(r'^# (.*)$', r'<b><font size="16">\1</font></b>', text, flags=re.MULTILINE)
    text = re.sub(r'^## (.*)$', r'<b><font size="14">\1</font></b>', text, flags=re.MULTILINE)
    text = re.sub(r'^### (.*)$', r'<b><font size="12">\1</font></b>', text, flags=re.MULTILINE)
    text = re.sub(r'^#### (.*)$', r'<b><font size="11">\1</font></b>', text, flags=re.MULTILINE)
    text = re.sub(r'^##### (.*)$', r'<b><font size="10">\1</font></b>', text, flags=re.MULTILINE)
    text = re.sub(r'^###### (.*)$', r'<b><font size="9">\1</font></b>', text, flags=re.MULTILINE)
    
    # å¤„ç†åŠ ç²—
    text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', text)
    
    # å¤„ç†æ–œä½“
    text = re.sub(r'\*(.*?)\*', r'<i>\1</i>', text)
    
    # å¤„ç†æ— åºåˆ—è¡¨ - ä½¿ç”¨HTMLåˆ—è¡¨æ ‡ç­¾
    text = re.sub(r'^-\s+(.*)$', r'<li>\1</li>', text, flags=re.MULTILINE)
    text = re.sub(r'^\*\s+(.*)$', r'<li>\1</li>', text, flags=re.MULTILINE)
    
    # å¤„ç†æœ‰åºåˆ—è¡¨ - ä½¿ç”¨HTMLåˆ—è¡¨æ ‡ç­¾
    def replace_ordered_list(match):
        return f'<li value="{match.group(1)}">.{match.group(2)}</li>'
    
    text = re.sub(r'^(\d+)\.\s+(.*)$', replace_ordered_list, text, flags=re.MULTILINE)
    
    # åŒ…è£¹åˆ—è¡¨é¡¹åœ¨ulæˆ–olæ ‡ç­¾ä¸­
    lines = text.split('\n')
    in_list = False
    list_type = None  # 'ul' æˆ– 'ol'
    processed_lines = []
    
    for line in lines:
        if line.startswith('<li>') or line.startswith('<li value='):
            if not in_list:
                # ç¡®å®šåˆ—è¡¨ç±»å‹
                if line.startswith('<li value='):
                    list_type = 'ol'
                    processed_lines.append(f'<{list_type}>')
                else:
                    list_type = 'ul'
                    processed_lines.append(f'<{list_type}>')
                in_list = True
            processed_lines.append(line)
        else:
            if in_list:
                processed_lines.append(f'</{list_type}>')
                in_list = False
                list_type = None
            processed_lines.append(line)
    
    # å¤„ç†æœ€åå¯èƒ½è¿˜åœ¨åˆ—è¡¨ä¸­çš„æƒ…å†µ
    if in_list:
        processed_lines.append(f'</{list_type}>')
    
    text = '\n'.join(processed_lines)
    
    # å¤„ç†åˆ†å‰²çº¿
    text = re.sub(r'^---+\s*$', r'<hr/>', text, flags=re.MULTILINE)
    text = re.sub(r'^\*\*\*+\s*$', r'<hr/>', text, flags=re.MULTILINE)
    
    # å¤„ç†æ¢è¡Œ
    text = text.replace('\n', '<br/>')
    
    return text

def create_docx_report(diagnosis_class, confidence_level, analysis_results, llm_report):
    # åˆ›å»ºWordæ–‡æ¡£
    doc = Document()
    
    # è®¾ç½®é»˜è®¤å­—ä½“ï¼ˆå°è¯•ä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼‰
    try:
        # è®¾ç½®å…¨å±€å­—ä½“
        style = doc.styles['Normal']
        font = style.font
        font.name = 'SimSun'  # å®‹ä½“
        font.size = Pt(12)
        # è®¾ç½®è¥¿æ–‡å­—ä½“
        font.name = 'Times New Roman'
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        r = style._element.rPr
        rFonts = r.get_or_add_rFonts()
        rFonts.set(qn('w:eastAsia'), 'SimSun')
    except:
        # å¦‚æœè®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
        pass
    
    # æ·»åŠ æ ‡é¢˜
    title = doc.add_heading('ç”µæœºæ•…éšœè¯Šæ–­æŠ¥å‘Š', level=0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    title_run = title.runs[0]
    title_run.font.size = Pt(16)
    title_run.font.bold = True
    
    # æ·»åŠ ç©ºè¡Œ
    doc.add_paragraph()
    
    # è¯Šæ–­ä¿¡æ¯
    subtitle = doc.add_heading('è¯Šæ–­ç»“æœ', level=1)
    subtitle_run = subtitle.runs[0]
    subtitle_run.font.size = Pt(14)
    subtitle_run.font.bold = True
    
    diagnosis_para = doc.add_paragraph()
    diagnosis_para.add_run('æ•…éšœç±»å‹: ').bold = True
    diagnosis_para.add_run(f'{diagnosis_class}')
    
    confidence_para = doc.add_paragraph()
    confidence_para.add_run('ç½®ä¿¡åº¦: ').bold = True
    confidence_para.add_run(f'{confidence_level*100:.2f}%')
    
    # æ·»åŠ ç©ºè¡Œ
    doc.add_paragraph()
    
    # åˆ†æç»“æœ
    analysis_title = doc.add_heading('ä¿¡å·åˆ†æ', level=1)
    analysis_title_run = analysis_title.runs[0]
    analysis_title_run.font.size = Pt(14)
    analysis_title_run.font.bold = True
    
    # åˆ›å»ºåˆ†æç»“æœè¡¨æ ¼
    table = doc.add_table(rows=1, cols=2)
    table.style = 'Table Grid'
    
    # è®¾ç½®è¡¨å¤´
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'å‚æ•°'
    hdr_cells[1].text = 'å€¼'
    
    # è®¾ç½®è¡¨å¤´æ ¼å¼
    for cell in hdr_cells:
        paragraph = cell.paragraphs[0]
        paragraph.runs[0].font.bold = True
        cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # æ·»åŠ æ•°æ®è¡Œ
    for key, value in analysis_results.items():
        row_cells = table.add_row().cells
        row_cells[0].text = str(key)
        row_cells[1].text = str(value)
    
    # æ·»åŠ ç©ºè¡Œ
    doc.add_paragraph()
    
    # æ·»åŠ AIåˆ†ææ ‡é¢˜
    analysis_title = doc.add_heading('AIåˆ†æä¸å»ºè®®', level=1)
    analysis_title_run = analysis_title.runs[0]
    analysis_title_run.font.size = Pt(14)
    analysis_title_run.font.bold = True
    
    # å¤„ç†LLMæŠ¥å‘Šå†…å®¹
    processed_report = process_markdown_for_docx(llm_report)
    
    # åˆ†å‰²å¤„ç†åçš„æŠ¥å‘Šä¸ºæ®µè½
    sections = processed_report.split('[NEWLINE][NEWLINE]')
    
    for section in sections:
        section = section.strip()
        if not section:
            continue
            
        # å¤„ç†åˆ†å‰²çº¿
        if section == '[HR]':
            # æ·»åŠ åˆ†å‰²çº¿ï¼ˆä½¿ç”¨æ®µè½è¾¹æ¡†ï¼‰
            para = doc.add_paragraph()
            para_format = para.paragraph_format
            
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„è¾¹æ¡†æ ·å¼è®¾ç½®
            from docx.enum.table import WD_TABLE_ALIGNMENT
            from docx.enum.text import WD_UNDERLINE
            
            # ä½¿ç”¨ç®€å•çš„æ–¹æ³•æ·»åŠ åˆ†å‰²çº¿
            run = para.add_run("â€•" * 50)  # ä½¿ç”¨é•¿ç ´æŠ˜å·ä½œä¸ºåˆ†å‰²çº¿
            run.font.color.rgb = RGBColor(200, 200, 200)  # ç°è‰²
            run.font.size = Pt(10)
            para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            continue
            
        # å¤„ç†æ ‡é¢˜
        if section.startswith('æ ‡é¢˜1: '):
            title = doc.add_heading(section[4:].strip(), level=1)
            title_run = title.runs[0]
            title_run.font.size = Pt(16)
            title_run.font.bold = True
        elif section.startswith('æ ‡é¢˜2: '):
            title = doc.add_heading(section[4:].strip(), level=2)
            title_run = title.runs[0]
            title_run.font.size = Pt(14)
            title_run.font.bold = True
        elif section.startswith('æ ‡é¢˜3: '):
            title = doc.add_heading(section[4:].strip(), level=3)
            title_run = title.runs[0]
            title_run.font.size = Pt(12)
            title_run.font.bold = True
        elif section.startswith('æ ‡é¢˜4: '):
            title = doc.add_heading(section[4:].strip(), level=4)
            title_run = title.runs[0]
            title_run.font.size = Pt(11)
            title_run.font.bold = True
        elif section.startswith('æ ‡é¢˜5: '):
            title = doc.add_heading(section[4:].strip(), level=5)
            title_run = title.runs[0]
            title_run.font.size = Pt(10)
            title_run.font.bold = True
        elif section.startswith('æ ‡é¢˜6: '):
            title = doc.add_heading(section[4:].strip(), level=6)
            title_run = title.runs[0]
            title_run.font.size = Pt(9)
            title_run.font.bold = True
        else:
            # å¤„ç†æ™®é€šæ®µè½
            paragraph = doc.add_paragraph()
            paragraph.paragraph_format.space_after = Pt(10)
            
            # å¤„ç†åŠ ç²—å’Œæ–œä½“æ ‡è®°
            parts = re.split(r'(\[BOLD\].*?\[/BOLD\]|\[ITALIC\].*?\[/ITALIC\])', section)
            for part in parts:
                if part.startswith('[BOLD]') and part.endswith('[/BOLD]'):
                    run = paragraph.add_run(part[6:-7])
                    run.bold = True
                elif part.startswith('[ITALIC]') and part.endswith('[/ITALIC]'):
                    run = paragraph.add_run(part[8:-9])
                    run.italic = True
                else:
                    # å¤„ç†æ™®é€šæ–‡æœ¬ä¸­çš„æ¢è¡Œ
                    sub_parts = part.split('[NEWLINE]')
                    for i, sub_part in enumerate(sub_parts):
                        if i > 0:
                            paragraph.add_run().add_break()  # æ·»åŠ æ¢è¡Œ
                        paragraph.add_run(sub_part)
    
    # æ·»åŠ é¡µè„š
    doc.add_paragraph()
    footer = doc.add_paragraph()
    footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
    footer_run = footer.add_run(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    footer_run.font.color.rgb = RGBColor(128, 128, 128)  # ç°è‰²
    footer_run.font.size = Pt(10)
    
    # ä¿å­˜åˆ°å­—èŠ‚æµ
    buffer = io.BytesIO()
    doc.save(buffer)
    
    # è·å–å­—èŠ‚æ•°æ®
    docx_data = buffer.getvalue()
    buffer.close()
    
    return docx_data

# åˆ›å»ºPDFæŠ¥å‘Šï¼ˆä½¿ç”¨ReportLabï¼‰- é’ˆå¯¹Streamlit Cloudä¼˜åŒ–
def create_pdf_report(diagnosis_class, confidence_level, analysis_results, llm_report):
    # åˆ›å»ºå­—èŠ‚ç¼“å†²åŒº
    buffer = io.BytesIO()
    
    # åˆ›å»ºPDFæ–‡æ¡£
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    
    # æ•…äº‹å…ƒç´ åˆ—è¡¨
    story = []
    
    # è·å–æ ·å¼
    styles = getSampleStyleSheet()
    
    # å°è¯•æ³¨å†Œä¸­æ–‡å­—ä½“
    font_name = 'Helvetica'  # é»˜è®¤å­—ä½“
    
    try:
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨é¡¹ç›®ä¸­çš„å­—ä½“æ–‡ä»¶
        font_paths = [
            "./fonts/simhei.ttf"
        ]
        
        font_registered = False
        for font_path in font_paths:
            try:
                if os.path.exists(font_path):
                    pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                    font_name = 'ChineseFont'
                    font_registered = True
                    st.success(f"æˆåŠŸåŠ è½½å­—ä½“: {font_path}")
                    break
            except Exception as e:
                st.warning(f"å­—ä½“åŠ è½½å¤±è´¥ {font_path}: {e}")
                continue
        
        # æ–¹æ³•2: å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“ (é€‚ç”¨äºStreamlit Cloudçš„Linuxç¯å¢ƒ)
        if not font_registered:
            system_fonts = [
                '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',
                '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            ]
            
            for font_path in system_fonts:
                try:
                    if os.path.exists(font_path):
                        pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                        font_name = 'ChineseFont'
                        font_registered = True
                        st.success(f"æˆåŠŸåŠ è½½ç³»ç»Ÿå­—ä½“: {font_path}")
                        break
                except Exception as e:
                    st.warning(f"ç³»ç»Ÿå­—ä½“åŠ è½½å¤±è´¥ {font_path}: {e}")
                    continue
        
        # æ–¹æ³•3: ä½¿ç”¨Base64ç¼–ç çš„å­—ä½“ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        if not font_registered:
            # è¿™é‡Œå¯ä»¥ä½¿ç”¨Base64ç¼–ç çš„å­—ä½“æ•°æ®
            # ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é¢„å…ˆå°†å­—ä½“æ–‡ä»¶è½¬æ¢ä¸ºBase64å­—ç¬¦ä¸²
            font_data_base64 = None  # è¿™é‡Œåº”è¯¥æ˜¯æ‚¨çš„Base64ç¼–ç å­—ä½“æ•°æ®
            
            if font_data_base64:
                try:
                    # åˆ›å»ºä¸´æ—¶å­—ä½“æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.ttf') as tmp:
                        tmp.write(base64.b64decode(font_data_base64))
                        font_path = tmp.name
                    
                    # æ³¨å†Œå­—ä½“
                    pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                    font_name = 'ChineseFont'
                    font_registered = True
                    st.success("æˆåŠŸåŠ è½½Base64å­—ä½“")
                except Exception as e:
                    st.warning(f"Base64å­—ä½“åŠ è½½å¤±è´¥: {e}")
    
    except Exception as e:
        st.error(f"å­—ä½“è®¾ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        font_name = 'Helvetica'  # å›é€€åˆ°é»˜è®¤å­—ä½“
    
    # åˆ›å»ºä¸­æ–‡å­—ä½“æ ·å¼
    if font_name != 'Helvetica':
        # æ·»åŠ ä¸­æ–‡æ ·å¼
        styles.add(ParagraphStyle(
            name='ChineseTitle',
            parent=styles['Title'],
            fontName=font_name,
            fontSize=16,
            spaceAfter=30,
        ))
        
        styles.add(ParagraphStyle(
            name='ChineseHeading',
            parent=styles['Heading1'],
            fontName=font_name,
            fontSize=14,
            spaceAfter=12,
        ))
        
        styles.add(ParagraphStyle(
            name='ChineseBody',
            parent=styles['BodyText'],
            fontName=font_name,
            fontSize=10,
            spaceAfter=6,
        ))
        
        # ä½¿ç”¨ä¸­æ–‡æ ·å¼
        title_style = styles['ChineseTitle']
        heading_style = styles['ChineseHeading']
        body_style = styles['ChineseBody']
    else:
        # ä½¿ç”¨é»˜è®¤æ ·å¼
        title_style = styles['Title']
        heading_style = styles['Heading1']
        body_style = styles['BodyText']
        st.warning("ä½¿ç”¨é»˜è®¤å­—ä½“Helveticaï¼Œä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤º")
    
    # åˆ›å»ºæ ·å¼
    styles = getSampleStyleSheet()
    
    # è‡ªå®šä¹‰æ ‡é¢˜æ ·å¼
    title_style = ParagraphStyle(
        'CustomTitle',
        fontName=font_name,
        fontSize=16,
        alignment=TA_CENTER,
        spaceAfter=20
    )
    
    # è‡ªå®šä¹‰å­æ ‡é¢˜æ ·å¼
    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        fontName=font_name,
        fontSize=14,
        textColor=colors.black,
        spaceAfter=10
    )
    
    # è‡ªå®šä¹‰æ­£æ–‡æ ·å¼ - æ·»åŠ ä¸­æ–‡æ’ç‰ˆæ”¯æŒ
    body_style = ParagraphStyle(
        'CustomBody',
        fontName=font_name,
        fontSize=12,
        leading=14,
        alignment=TA_JUSTIFY,
        spaceAfter=10,
        wordWrap='CJK'  # æ·»åŠ ä¸­æ–‡æ¢è¡Œæ”¯æŒ
    )
    
    # è‡ªå®šä¹‰ä¿¡æ¯æ ·å¼
    info_style = ParagraphStyle(
        'CustomInfo',
        fontName=font_name,
        fontSize=12,
        textColor=colors.grey,
        spaceAfter=5
    )
    
    # æ„å»ºå†…å®¹
    story = []
    
    # æ ‡é¢˜
    story.append(Paragraph('ç”µæœºæ•…éšœè¯Šæ–­æŠ¥å‘Š', title_style))
    story.append(Spacer(1, 20))
    
    # è¯Šæ–­ä¿¡æ¯
    story.append(Paragraph('è¯Šæ–­ç»“æœ', subtitle_style))
    story.append(Paragraph(f'æ•…éšœç±»å‹: {diagnosis_class}', info_style))
    story.append(Paragraph(f'ç½®ä¿¡åº¦: {confidence_level*100:.2f}%', info_style))
    story.append(Spacer(1, 15))
    
    # åˆ†æç»“æœ
    story.append(Paragraph('ä¿¡å·åˆ†æ', subtitle_style))
    
    # åˆ›å»ºåˆ†æç»“æœè¡¨æ ¼
    analysis_data = [['å‚æ•°', 'å€¼']]
    for key, value in analysis_results.items():
        analysis_data.append([key, str(value)])
    
    analysis_table = Table(analysis_data, colWidths=[2*inch, 3*inch])
    analysis_table.setStyle(TableStyle([
        ('FONT', (0, 0), (-1, -1), font_name, 10),
        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.black),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONT', (0, 0), (-1, 0), font_name, 12),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), colors.white),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    
    story.append(analysis_table)
    story.append(Spacer(1, 20))
    
    # æ·»åŠ AIåˆ†ææ ‡é¢˜
    ai_title_style = ParagraphStyle(
        'AITitle',
        fontName=font_name,
        fontSize=14,
        textColor=colors.black,
        spaceAfter=10,
        spaceBefore=20
    )
    story.append(Paragraph('AIåˆ†æä¸å»ºè®®', ai_title_style))
    
    # å¤„ç†LLMæŠ¥å‘Šå†…å®¹
    # å°†æŠ¥å‘Šå†…å®¹æŒ‰è¡Œåˆ†å‰²ï¼Œå¤„ç†åˆ—è¡¨é¡¹
    report_lines = llm_report.split('\n')
    
    # ç”¨äºè·Ÿè¸ªåˆ—è¡¨çŠ¶æ€
    in_list = False
    list_items = []
    list_counter = 1  # åˆ—è¡¨é¡¹è®¡æ•°å™¨
    
    def flush_list(reset_counter=False):
        """å°†å½“å‰åˆ—è¡¨é¡¹æ·»åŠ åˆ°storyä¸­"""
        nonlocal list_items, list_counter
        if not list_items:
            return
            
        # åˆ›å»ºè¡¨æ ¼æ¥æ˜¾ç¤ºåˆ—è¡¨é¡¹ï¼Œç¡®ä¿åºå·å’Œå†…å®¹åœ¨åŒä¸€è¡Œ
        table_data = []
        for i, item in enumerate(list_items, list_counter):
            # å¤„ç†åŠ ç²—æ–‡æœ¬
            item = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', item)
            
            # å°†åºå·å’Œå†…å®¹æ”¾åœ¨åŒä¸€è¡Œ
            table_data.append([
                Paragraph(f"({i})", body_style),
                Paragraph(item, body_style)
            ])
        
        # åˆ›å»ºè¡¨æ ¼ - ä½¿ç”¨ç´§å‡‘å¸ƒå±€
        list_table = Table(table_data, colWidths=[0.3*inch, 6.2*inch])
        list_table.setStyle(TableStyle([
            ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ('ALIGN', (0, 0), (0, -1), 'RIGHT'),
            ('ALIGN', (1, 0), (1, -1), 'LEFT'),
            ('LEFTPADDING', (0, 0), (0, -1), 1),
            ('RIGHTPADDING', (0, 0), (0, -1), 1),
            ('LEFTPADDING', (1, 0), (1, -1), 1),
            ('RIGHTPADDING', (1, 0), (1, -1), 1),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 2),
            ('TOPPADDING', (0, 0), (-1, -1), 2),
        ]))
        
        story.append(list_table)
        story.append(Spacer(1, 4))
        
        list_items = []
        
        # å¦‚æœéœ€è¦é‡ç½®è®¡æ•°å™¨ï¼Œåˆ™é‡ç½®ä¸º1
        if reset_counter:
            list_counter = 1
        else:
            # å¦åˆ™æ›´æ–°è®¡æ•°å™¨
            list_counter += len(table_data)
    
    # æ–‡æœ¬æ¸…ç†å‡½æ•°
    def clean_text(text):
        """æ¸…ç†æ–‡æœ¬ï¼Œç¡®ä¿ä¸ä»¥æ ‡ç‚¹ç¬¦å·å¼€å¤´"""
        if not text:
            return text
        
        # ç§»é™¤å¼€å¤´çš„æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'^[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;:\s]+', '', text)
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    # ä¸­æ–‡æ–‡æœ¬å¤„ç†å‡½æ•°
    def process_chinese_text(text):
        """å¤„ç†ä¸­æ–‡æ–‡æœ¬ï¼Œä¼˜åŒ–æ’ç‰ˆ"""
        if not text:
            return text
        
        # å¤„ç†å¸¸è§çš„æ ‡ç‚¹ç¬¦å·è¿æ¥é—®é¢˜
        # ç¡®ä¿æ ‡ç‚¹ç¬¦å·è·Ÿéšåœ¨å‰é¢çš„æ–‡æœ¬åé¢
        text = re.sub(r'([^ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])\s*([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])', r'\1\2', text)
        
        # å¤„ç†è‹±æ–‡å’Œæ•°å­—ä¸ä¸­æ–‡çš„é—´è·
        text = re.sub(r'([a-zA-Z0-9])\s*([\u4e00-\u9fff])', r'\1 \2', text)
        text = re.sub(r'([\u4e00-\u9fff])\s*([a-zA-Z0-9])', r'\1 \2', text)
        
        return text
    
    # æ®µè½ç¼“å†²åŒºï¼Œç”¨äºåˆå¹¶å¤šè¡Œæ–‡æœ¬å½¢æˆå®Œæ•´æ®µè½
    paragraph_buffer = []
    
    def flush_paragraph():
        """å°†ç¼“å†²åŒºä¸­çš„æ–‡æœ¬åˆå¹¶ä¸ºä¸€ä¸ªæ®µè½å¹¶æ·»åŠ åˆ°storyä¸­"""
        nonlocal paragraph_buffer
        if not paragraph_buffer:
            return
        
        # åˆå¹¶ç¼“å†²åŒºä¸­çš„æ–‡æœ¬
        paragraph_text = ''.join(paragraph_buffer)  # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥ï¼Œé¿å…æ·»åŠ é¢å¤–ç©ºæ ¼
        paragraph_text = clean_text(paragraph_text)
        paragraph_text = process_chinese_text(paragraph_text)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
        
        if paragraph_text:
            # å¤„ç†åŠ ç²—æ–‡æœ¬
            paragraph_text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', paragraph_text)
            story.append(Paragraph(paragraph_text, body_style))
            story.append(Spacer(1, 6))
        
        paragraph_buffer = []
    
    for line in report_lines:
        line = line.strip()
        if not line:
            # ç©ºè¡Œè¡¨ç¤ºæ®µè½ç»“æŸ
            flush_paragraph()
            if in_list:
                flush_list()
                in_list = False
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨é¡¹
        is_list_item = line.startswith('- ') or line.startswith('* ') or re.match(r'^\d+\.\s+', line)
        
        if is_list_item:
            # åˆ·æ–°å½“å‰æ®µè½
            flush_paragraph()
            
            # å¼€å§‹æˆ–ç»§ç»­åˆ—è¡¨
            in_list = True
            
            # æå–åˆ—è¡¨é¡¹å†…å®¹
            if line.startswith('- ') or line.startswith('* '):
                list_item = line[2:].strip()  # ç§»é™¤ "- " æˆ– "* "
            else:
                # å¯¹äºæœ‰åºåˆ—è¡¨ï¼Œç§»é™¤æ•°å­—å’Œç‚¹
                list_item = re.sub(r'^\d+\.\s+', '', line)
                
            # æ¸…ç†åˆ—è¡¨é¡¹æ–‡æœ¬
            list_item = clean_text(list_item)
            list_item = process_chinese_text(list_item)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
            if list_item:
                list_items.append(list_item)
        else:
            # å¦‚æœä¸æ˜¯åˆ—è¡¨é¡¹ï¼Œåˆ·æ–°å½“å‰åˆ—è¡¨
            if in_list:
                flush_list()
                in_list = False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
            is_title = False
            if line.startswith('#### '):
                # å››çº§æ ‡é¢˜
                is_title = True
                flush_paragraph()  # åˆ·æ–°å½“å‰æ®µè½
                title_text = line[5:].strip()
                title_text = clean_text(title_text)
                title_text = process_chinese_text(title_text)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
                title_text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', title_text)
                title_style = ParagraphStyle(
                    'ReportTitle4',
                    fontName=font_name,
                    fontSize=11,
                    textColor=colors.black,
                    spaceAfter=6,
                    spaceBefore=12
                )
                story.append(Paragraph(title_text, title_style))
            elif line.startswith('### '):
                # ä¸‰çº§æ ‡é¢˜
                is_title = True
                flush_paragraph()  # åˆ·æ–°å½“å‰æ®µè½
                title_text = line[4:].strip()
                title_text = clean_text(title_text)
                title_text = process_chinese_text(title_text)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
                title_text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', title_text)
                title_style = ParagraphStyle(
                    'ReportTitle3',
                    fontName=font_name,
                    fontSize=12,
                    textColor=colors.black,
                    spaceAfter=6,
                    spaceBefore=12
                )
                story.append(Paragraph(title_text, title_style))
            elif line.startswith('## '):
                # äºŒçº§æ ‡é¢˜
                is_title = True
                flush_paragraph()  # åˆ·æ–°å½“å‰æ®µè½
                title_text = line[3:].strip()
                title_text = clean_text(title_text)
                title_text = process_chinese_text(title_text)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
                title_text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', title_text)
                title_style = ParagraphStyle(
                    'ReportTitle2',
                    fontName=font_name,
                    fontSize=14,
                    textColor=colors.black,
                    spaceAfter=6,
                    spaceBefore=12
                )
                story.append(Paragraph(title_text, title_style))
            elif line.startswith('# '):
                # ä¸€çº§æ ‡é¢˜
                is_title = True
                flush_paragraph()  # åˆ·æ–°å½“å‰æ®µè½
                title_text = line[2:].strip()
                title_text = clean_text(title_text)
                title_text = process_chinese_text(title_text)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
                title_text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', title_text)
                title_style = ParagraphStyle(
                    'ReportTitle1',
                    fontName=font_name,
                    fontSize=16,
                    textColor=colors.black,
                    spaceAfter=6,
                    spaceBefore=12
                )
                story.append(Paragraph(title_text, title_style))
            elif line.startswith('---') or line.startswith('***'):
                # åˆ†å‰²çº¿
                flush_paragraph()  # åˆ·æ–°å½“å‰æ®µè½
                story.append(Spacer(1, 12))
                hr = Table([['']], colWidths=[6*inch], rowHeights=[1])
                hr.setStyle(TableStyle([
                    ('LINEABOVE', (0, 0), (-1, -1), 1, colors.grey),
                ]))
                story.append(hr)
                story.append(Spacer(1, 12))
            else:
                # æ™®é€šæ®µè½ - æ·»åŠ åˆ°ç¼“å†²åŒº
                line = process_chinese_text(line)  # å¤„ç†ä¸­æ–‡æ’ç‰ˆ
                paragraph_buffer.append(line)
            
            # å¦‚æœæ˜¯æ ‡é¢˜ï¼Œé‡ç½®åˆ—è¡¨è®¡æ•°å™¨
            if is_title:
                list_counter = 1
    
    # å¤„ç†æœ€åå¯èƒ½è¿˜åœ¨ç¼“å†²åŒºæˆ–åˆ—è¡¨ä¸­çš„å†…å®¹
    flush_paragraph()
    if in_list:
        flush_list()
    
    # æ·»åŠ é¡µè„š
    story.append(Spacer(1, 20))
    footer_style = ParagraphStyle(
        'CustomFooter',
        fontName=font_name,
        fontSize=10,
        alignment=TA_CENTER,
        textColor=colors.grey
    )
    story.append(Paragraph(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", footer_style))
    
    # æ„å»ºPDF
    doc.build(story)
    
    # è·å–PDFæ•°æ®
    pdf_data = buffer.getvalue()
    buffer.close()
    
    return pdf_data

def generate_diagnostic_report(diagnosis_class, confidence_level, analysis_results):
    # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è¯Šæ–­ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ç½®
    if 'diagnosis_results' not in st.session_state or st.session_state['diagnosis_results'] is None:
        st.session_state['diagnosis_results'] = {
            'diagnosis_class': diagnosis_class,
            'confidence_level': confidence_level,
            'analysis_results': analysis_results,
            'report': None,
            'pdf_data': None,  # æ–°å¢ï¼šä¿å­˜PDFæ•°æ®
            'docx_data': None  # æ–°å¢ï¼šä¿å­˜DOCXæ•°æ®
        }
    else:
        # å¦‚æœå·²ç»å­˜åœ¨ï¼Œåˆ™æ›´æ–°è¯Šæ–­ç»“æœï¼ˆç¡®ä¿æ˜¯æœ€æ–°çš„ï¼‰
        st.session_state['diagnosis_results'].update({
            'diagnosis_class': diagnosis_class,
            'confidence_level': confidence_level,
            'analysis_results': analysis_results
        })

    # å‡†å¤‡æ•…éšœæ‰‹å†Œä¿¡æ¯ - ä½¿ç”¨å‘é‡çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹
    manual_info = ""
    if ('vector_index' in st.session_state and 
        'vector_chunks' in st.session_state and 
        st.session_state['vector_index'] is not None and
        st.session_state['vector_chunks'] is not None):
        
        # ä»å‘é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸è¯Šæ–­ç»“æœç›¸å…³çš„å†…å®¹
        embedding_model = load_embedding_model()
        if embedding_model:
            # ä½¿ç”¨è¯Šæ–­ç»“æœä½œä¸ºæŸ¥è¯¢
            query = f"{diagnosis_class} {analysis_results}"
            relevant_chunks = search_vector_knowledge_base(
                query, 
                st.session_state['vector_index'],
                st.session_state['vector_chunks'],
                embedding_model,
                top_k=3
            )
            
            if relevant_chunks:
                manual_info = "ä»¥ä¸‹æ˜¯æ ¹æ®æ•…éšœæ‰‹å†Œæ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n"
                for i, chunk in enumerate(relevant_chunks):
                    manual_info += f"\n[ç›¸å…³æ®µè½ {i+1}, ç›¸ä¼¼åº¦: {chunk['score']:.3f}]\n{chunk['text']}\n"
    
    llm_prompt = f"""
    å·²çŸ¥æŒ¯åŠ¨ä¿¡å·ç»è¿‡CNNæ¨¡å‹è¯Šæ–­ï¼Œç»“æœå¦‚ä¸‹ï¼š
    - è¯Šæ–­ç»“æœ: {diagnosis_class}
    - ç½®ä¿¡åº¦: {confidence_level*100:.2f}%
    - ä¿¡å·ç»Ÿè®¡: {analysis_results}

    {manual_info}

    è¯·ä½ ä½œä¸ºæœºæ¢°è®¾å¤‡æ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œæ ¹æ®ä¸Šè¿°è¯Šæ–­ç»“æœå’Œæ•…éšœæ‰‹å†Œå‚è€ƒï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€è¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Šï¼ŒæŠ¥å‘Šå†…å®¹åº”åŒ…æ‹¬ï¼š
    1. æ•…éšœè§£é‡Šï¼šè¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆå¯èƒ½æ˜¯è¯¥æ•…éšœï¼Œä»æœºæ¢°åŸç†è§’åº¦è§£é‡Š
    2. å¯èƒ½çš„æˆå› ï¼šåˆ†æå¯¼è‡´æ­¤ç±»æ•…éšœçš„å¤šç§å¯èƒ½åŸå› 
    3. å»ºè®®çš„å¤„ç†æªæ–½ï¼šæä¾›å…·ä½“ã€å¯è¡Œçš„ç»´ä¿®å’Œå¤„ç†å»ºè®®ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    4. é¢„é˜²æªæ–½ï¼šå»ºè®®å¦‚ä½•é¢„é˜²æ­¤ç±»æ•…éšœå†æ¬¡å‘ç”Ÿ

    è¯·ç¡®ä¿æŠ¥å‘Šä¸“ä¸šã€å‡†ç¡®ä¸”æ˜“äºç†è§£ï¼Œä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
    """
    
    st.subheader("ğŸ“Š AIå¤§æ¨¡å‹è§£é‡Šä¸å»ºè®®")
    
    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
    messages = [{"role": "user", "content": llm_prompt}]

    report = call_ai_api(messages)
    
    if report:
        st.success("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        st.write(report)

        # æ›´æ–°session_stateä¸­çš„æŠ¥å‘Šå†…å®¹
        st.session_state['diagnosis_results']['report'] = report
        
        try:
            # åˆ›å»ºPDFæŠ¥å‘Šå¹¶ä¿å­˜æ•°æ®
            pdf_data = create_pdf_report(
                diagnosis_class, 
                confidence_level, 
                analysis_results,
                report
            )
            st.session_state['diagnosis_results']['pdf_data'] = pdf_data
            
            # åˆ›å»ºDOCXæŠ¥å‘Šå¹¶ä¿å­˜æ•°æ®
            docx_data = create_docx_report(
                diagnosis_class, 
                confidence_level, 
                analysis_results,
                report
            )
            st.session_state['diagnosis_results']['docx_data'] = docx_data
            
            # æ˜¾ç¤ºä¸‹è½½æŒ‰é’®
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    label="ä¸‹è½½è¯Šæ–­æŠ¥å‘Š (PDF)",
                    data=pdf_data,
                    file_name=f"è®¾å¤‡è¯Šæ–­æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                    mime="application/pdf"
                )
            with col2:
                st.download_button(
                    label="ä¸‹è½½è¯Šæ–­æŠ¥å‘Š (DOCX)",
                    data=docx_data,
                    file_name=f"è®¾å¤‡è¯Šæ–­æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )
            
        except Exception as e:
            st.error(f"ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            st.info("å¦‚æœæ‚¨éœ€è¦PDFæˆ–DOCXç‰ˆæœ¬ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿå®‰è£…äº†å¿…è¦çš„åº“å’Œå­—ä½“")
    else:
        st.error("ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®æˆ–ç¨åé‡è¯•")

# åˆå§‹åŒ–session stateå˜é‡
def initialize_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰session stateå˜é‡"""
    if 'diagnosis_completed' not in st.session_state:
        st.session_state.diagnosis_completed = False
    if 'predicted_class' not in st.session_state:
        st.session_state.predicted_class = None
    if 'confidence_value' not in st.session_state:
        st.session_state.confidence_value = None
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = {}
    if 'show_report' not in st.session_state:
        st.session_state.show_report = False
    if 'diagnosis_results' not in st.session_state:
        st.session_state.diagnosis_results = None
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œå¸®åŠ©ç”¨æˆ·åˆ†æä¿¡å·å¹¶ç»™å‡ºè¯Šæ–­å»ºè®®ã€‚"}
        ]
    if 'manual_content' not in st.session_state:
        st.session_state.manual_content = None
    if 'vector_index' not in st.session_state:
        st.session_state.vector_index = None
    if 'vector_chunks' not in st.session_state:
        st.session_state.vector_chunks = None
    if 'processing_mode' not in st.session_state:
        st.session_state.processing_mode = "è‡ªåŠ¨"  # é»˜è®¤å€¼
    if 'avg_probabilities' not in st.session_state:
        st.session_state.avg_probabilities = None
    if 'preferred_api' not in st.session_state:
        st.session_state.preferred_api = "deepseek"  # é»˜è®¤ä¼˜å…ˆä½¿ç”¨DeepSeek

# è°ƒç”¨åˆå§‹åŒ–å‡½æ•°
initialize_session_state()

# è°ƒç”¨åŠ è½½æ¨¡å‹å‡½æ•°
model = load_model()

# æ·»åŠ ä¾§è¾¹æ ä¿¡æ¯
with st.sidebar:
    st.header("å…³äº")
    st.markdown("""
    æœ¬ç³»ç»ŸåŸºäºæ·±åº¦å­¦ä¹ CNNæ¨¡å‹ï¼Œç”¨äºç”µæœºè½´æ‰¿æ•…éšœè¯Šæ–­ã€‚
    
    ### æ”¯æŒè¯Šæ–­çš„æ•…éšœç±»å‹:
    - æ­£å¸¸çŠ¶æ€
    - 7milå†…åœˆæ•…éšœ
    - 7milæ»šåŠ¨ä½“æ•…éšœ
    - 7milå¤–åœˆæ•…éšœ
    - 14milå†…åœˆæ•…éšœ
    - 14milæ»šåŠ¨ä½“æ•…éšœ
    - 14milå¤–åœˆæ•…éšœ
    - 21milå†…åœˆæ•…éšœ
    - 21milæ»šåŠ¨ä½“æ•…éšœ
    - 21milå¤–åœˆæ•…éšœ
    
    ### ä½¿ç”¨æ–¹æ³•:
    1. ä¸Šä¼ CSVæ ¼å¼çš„æŒ¯åŠ¨æ•°æ®æ–‡ä»¶
    2. é€‰æ‹©è¦åˆ†æçš„æ•°æ®åˆ—(å¦‚æœæœ‰å¤šä¸ªåˆ—)
    3. ç‚¹å‡»"å¼€å§‹æ•…éšœè¯Šæ–­"æŒ‰é’®
    4. æŸ¥çœ‹è¯Šæ–­ç»“æœå’Œå»ºè®®
    """)
    
    st.header("æ¨¡å‹ä¿¡æ¯")
    if model is not None:
        st.success("æ¨¡å‹å·²åŠ è½½")
        st.write(f"è®¾å¤‡: {'GPU' if device.type == 'cuda' else 'CPU'}")
    else:
        st.error("æ¨¡å‹æœªåŠ è½½")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("APIè®¾ç½®")
    
    # æ·»åŠ APIé€‰æ‹©é€‰é¡¹
    api_option = st.radio(
        "é€‰æ‹©ä¼˜å…ˆä½¿ç”¨çš„API",
        ["DeepSeek", "KIMI"],
        index=0 if st.session_state.preferred_api == "deepseek" else 1,
        help="é€‰æ‹©ä¼˜å…ˆä½¿ç”¨å“ªä¸ªAPIæ¥å›ç­”é—®é¢˜"
    )
    
    # æ›´æ–°é¦–é€‰çš„API
    st.session_state.preferred_api = "deepseek" if api_option == "DeepSeek" else "kimi"
    
    st.header("æ€§èƒ½è®¾ç½®")
    
    # æ·»åŠ æ€§èƒ½é…ç½®é€‰é¡¹
    chunk_size = st.slider("æ–‡æœ¬å—å¤§å°", min_value=200, max_value=1000, value=500, step=100,
                         help="è¾ƒå°çš„å€¼æé«˜ç²¾åº¦ä½†å¢åŠ å¤„ç†æ—¶é—´ï¼Œè¾ƒå¤§çš„å€¼å‡å°‘å¤„ç†æ—¶é—´ä½†å¯èƒ½é™ä½ç²¾åº¦")
    
    overlap_size = st.slider("é‡å å¤§å°", min_value=0, max_value=200, value=50, step=10,
                           help="æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œæœ‰åŠ©äºä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§")
    
    # æ·»åŠ å‚æ•°éªŒè¯
    if overlap_size >= chunk_size:
        st.error("é”™è¯¯: é‡å å¤§å°å¿…é¡»å°äºæ–‡æœ¬å—å¤§å°")
        # è‡ªåŠ¨è°ƒæ•´é‡å å¤§å°
        overlap_size = min(overlap_size, chunk_size - 10)
        st.info(f"å·²è‡ªåŠ¨è°ƒæ•´é‡å å¤§å°ä¸º: {overlap_size}")
    
    batch_size = st.slider("æ‰¹é‡å¤„ç†å¤§å°", min_value=8, max_value=64, value=16, step=8,
                         help="è¾ƒå¤§çš„æ‰¹é‡æé«˜å¤„ç†é€Ÿåº¦ä½†éœ€è¦æ›´å¤šå†…å­˜")
    
    st.header("APIæµ‹è¯•")
    
    # æ·»åŠ DeepSeek APIæµ‹è¯•æŒ‰é’®
    if st.button("æµ‹è¯•DeepSeek APIè¿æ¥"):
        test_prompt = "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'ä»¥ç¡®è®¤APIè¿æ¥æ­£å¸¸ã€‚"
        test_messages = [{"role": "user", "content": test_prompt}]

        # ä¸´æ—¶æ¸…é™¤è¯Šæ–­ç»“æœï¼Œé¿å…å½±å“æµ‹è¯•
        original_diagnosis = st.session_state.get('diagnosis_results')
        st.session_state['diagnosis_results'] = None
        
        with st.spinner("æµ‹è¯•DeepSeek APIè¿æ¥ä¸­..."):
            res = call_deepseek_api(test_messages, max_tokens=50)
        
        # æ¢å¤åŸå§‹è¯Šæ–­ç»“æœ
        st.session_state['diagnosis_results'] = original_diagnosis

        if res and "è¿æ¥æˆåŠŸ" in res:
            st.success(f"DeepSeek APIè¿æ¥æµ‹è¯•æˆåŠŸï¼å“åº”: {res}")
        else:
            st.error(f"DeepSeek APIè¿æ¥æµ‹è¯•å¤±è´¥: {res}")
    
    # æ·»åŠ KIMI APIæµ‹è¯•æŒ‰é’®
    if st.button("æµ‹è¯•KIMI APIè¿æ¥"):
        test_prompt = "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'ä»¥ç¡®è®¤APIè¿æ¥æ­£å¸¸ã€‚"
        test_messages = [{"role": "user", "content": test_prompt}]

        # ä¸´æ—¶æ¸…é™¤è¯Šæ–­ç»“æœï¼Œé¿å…å½±å“æµ‹è¯•
        original_diagnosis = st.session_state.get('diagnosis_results')
        st.session_state['diagnosis_results'] = None
        
        with st.spinner("æµ‹è¯•KIMI APIè¿æ¥ä¸­..."):
            res = call_kimi_api(test_messages, max_tokens=50)
        
        # æ¢å¤åŸå§‹è¯Šæ–­ç»“æœ
        st.session_state['diagnosis_results'] = original_diagnosis

        if res and "è¿æ¥æˆåŠŸ" in res:
            st.success(f"KIMI APIè¿æ¥æµ‹è¯•æˆåŠŸï¼å“åº”: {res}")
        else:
            st.error(f"KIMI APIè¿æ¥æµ‹è¯•å¤±è´¥: {res}")
    
    st.header("æ•…éšœæ‰‹å†Œè®¾ç½®")
    manual_file = st.file_uploader("ä¸Šä¼ æ•…éšœæ‰‹å†Œ (PDF/TXT/DOCX)", type=["pdf", "txt", "docx"])
    
    if manual_file is not None:
        # æå–æ–‡æœ¬å†…å®¹
        manual_text = extract_text_from_file(manual_file)
        if manual_text:
            # é¢„å¤„ç†æ–‡æœ¬
            manual_text = preprocess_text(manual_text)
            
            st.session_state.manual_content = manual_text
            
            # æ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯
            text_length = len(manual_text)
            
            # ä¿®å¤åˆ†å—è®¡ç®—é€»è¾‘
            if chunk_size <= overlap_size:
                st.error("é”™è¯¯: æ–‡æœ¬å—å¤§å°å¿…é¡»å¤§äºé‡å å¤§å°")
                estimated_chunks = 1
            else:
                # è®¡ç®—é¢„è®¡åˆ†å—æ•° - ä¿®å¤åçš„å…¬å¼
                if text_length <= chunk_size:
                    estimated_chunks = 1
                else:
                    estimated_chunks = max(1, (text_length - overlap_size) // (chunk_size - overlap_size) + 1)
            
            st.info(f"æ–‡æœ¬é•¿åº¦: {text_length} å­—ç¬¦, é¢„è®¡åˆ†æˆ: {estimated_chunks} ä¸ªæ–‡æœ¬å—")
            
            # æ ¹æ®å¤„ç†æ¨¡å¼å†³å®šæ˜¯å¦åˆ›å»ºå‘é‡ç´¢å¼•
            if st.session_state.processing_mode == "ä»…æ–‡æœ¬æœç´¢" or (st.session_state.processing_mode == "è‡ªåŠ¨" and text_length > 200000):
                # å¯¹äºé•¿æ–‡æœ¬ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬æœç´¢
                chunks = []
                start = 0
                while start < len(manual_text):
                    end = min(start + chunk_size, len(manual_text))
                    chunk = manual_text[start:end]
                    chunks.append(chunk)
                    start = end - overlap_size  # è€ƒè™‘é‡å 
                
                st.session_state.vector_index = None
                st.session_state.vector_chunks = chunks
                st.info(f"å·²åˆ›å»º {len(chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µç”¨äºç®€å•æœç´¢")
            else:
                # åˆ›å»ºå‘é‡çŸ¥è¯†åº“ï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
                embedding_model = load_embedding_model()
                if embedding_model:
                    with st.spinner("æ­£åœ¨æ„å»ºå‘é‡çŸ¥è¯†åº“..."):
                        index, chunks = create_vector_knowledge_base_optimized(
                            manual_text, 
                            embedding_model,
                            chunk_size=chunk_size,
                            overlap=overlap_size,
                            batch_size=batch_size
                        )
                        st.session_state.vector_index = index
                        st.session_state.vector_chunks = chunks
                    
                    if index is not None:
                        st.success(f"æ•…éšœæ‰‹å†Œä¸Šä¼ æˆåŠŸ! å·²æ„å»º {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")
                    else:
                        st.warning("ä½¿ç”¨ç®€å•æ–‡æœ¬æœç´¢æ¨¡å¼ï¼ŒæŸäº›é«˜çº§åŠŸèƒ½å¯èƒ½å—é™")
                else:
                    st.error("åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬æœç´¢æ¨¡å¼")
                    # å³ä½¿æ²¡æœ‰åµŒå…¥æ¨¡å‹ï¼Œä¹Ÿåˆ›å»ºç®€å•çš„æ–‡æœ¬åº“
                    chunks = []
                    start = 0
                    while start < len(manual_text):
                        end = min(start + chunk_size, len(manual_text))
                        chunk = manual_text[start:end]
                        chunks.append(chunk)
                        start = end - overlap_size  # è€ƒè™‘é‡å 
                    
                    st.session_state.vector_index = None
                    st.session_state.vector_chunks = chunks
                    st.info(f"å·²åˆ›å»º {len(chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µç”¨äºç®€å•æœç´¢")
        else:
            st.error("æå–æ–‡æœ¬å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶ã€‚")
    elif 'manual_content' not in st.session_state:
        st.session_state.manual_content = None
        st.session_state.vector_index = None
        st.session_state.vector_chunks = None
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    if st.session_state.vector_index is not None:
        st.info(f"å·²åŠ è½½æ•…éšœæ‰‹å†Œï¼ŒåŒ…å« {len(st.session_state.vector_chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")
        if st.button("æ¸…é™¤æ•…éšœæ‰‹å†Œ", key="clear_manual_btn"):
            # å½»åº•æ¸…é™¤æ‰€æœ‰ä¸æ‰‹å†Œç›¸å…³çš„çŠ¶æ€
            st.session_state.manual_content = None
            st.session_state.vector_index = None
            st.session_state.vector_chunks = None
            
            # æ¸…é™¤åµŒå…¥æ¨¡å‹ç¼“å­˜ï¼ˆé‡è¦ï¼ï¼‰
            if 'load_embedding_model' in st.session_state:
                del st.session_state['load_embedding_model']
            
            # æ¸…é™¤è¯Šæ–­ç»“æœä¸­çš„æ‰‹å†Œå¼•ç”¨
            if 'diagnosis_results' in st.session_state:
                # ä¿ç•™è¯Šæ–­ç»“æœä½†ç§»é™¤æ‰‹å†Œå†…å®¹
                diagnosis = st.session_state['diagnosis_results']
                if diagnosis and 'manual_references' in diagnosis:
                    del diagnosis['manual_references']
            
            # æ¸…é™¤å¯¹è¯å†å²ä¸­å¯èƒ½åŒ…å«çš„æ‰‹å†Œå†…å®¹
            st.session_state["messages"] = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€åç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œå¸®åŠ©ç”¨æˆ·åˆ†æä¿¡å·å¹¶ç»™å‡ºè¯Šæ–­å»ºè®®ã€‚"}
            ]
            
            st.success("æ•…éšœæ‰‹å†Œå·²å½»åº•æ¸…é™¤!")
            st.rerun()
    else:
        st.warning("æœªä¸Šä¼ æ•…éšœæ‰‹å†Œ")

# åœ¨ä¾§è¾¹æ æ·»åŠ æ¸…é™¤ç¼“å­˜æŒ‰é’®
st.sidebar.header("é«˜çº§è®¾ç½®")
if st.sidebar.button("å¼ºåˆ¶æ¸…é™¤æ‰€æœ‰ç¼“å­˜", key="clear_all_cache_btn"):
    # æ¸…é™¤æ‰€æœ‰ç¼“å­˜å’Œä¼šè¯çŠ¶æ€
    keys_to_keep = []  # ä¿ç•™å¿…è¦çš„é”®
    keys_to_delete = [key for key in st.session_state.keys() if key not in keys_to_keep]
    
    for key in keys_to_delete:
        del st.session_state[key]
    
    # æ¸…é™¤èµ„æºç¼“å­˜
    if 'load_embedding_model' in st.session_state:
        del st.session_state['load_embedding_model']
    
    st.sidebar.success("æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤!")
    st.rerun()

# ä¸»ç•Œé¢
st.header("æŒ¯åŠ¨æ•°æ®åˆ†æ")

# æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
uploaded_file = st.file_uploader("é€‰æ‹©æŒ¯åŠ¨æ•°æ®æ–‡ä»¶ (CSVæ ¼å¼)", type=["csv"])

if uploaded_file is not None and model is not None:
    try:
        dataframe = pd.read_csv(uploaded_file)
        st.success("æ–‡ä»¶ä¸Šä¼ æˆåŠŸ!")
        
        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        st.subheader("æ•°æ®æ¦‚è§ˆ")
        col1, col2, col3 = st.columns(3)
        col1.metric("æ•°æ®è¡Œæ•°", dataframe.shape[0])
        col2.metric("æ•°æ®åˆ—æ•°", dataframe.shape[1])
        col3.metric("é‡‡æ ·ç‚¹æ•°", dataframe.shape[0])
        
        # é€‰æ‹©è¦åˆ†æçš„æ•°æ®åˆ—
        if dataframe.shape[1] > 1:
            selected_column = st.selectbox("é€‰æ‹©è¦åˆ†æçš„æ•°æ®åˆ—", dataframe.columns)
            vibration_data = dataframe[selected_column].values
        else:
            vibration_data = dataframe.iloc[:, 0].values
        
        # æ˜¾ç¤ºæ•°æ®æ³¢å½¢
        st.subheader("æŒ¯åŠ¨ä¿¡å·æ³¢å½¢")
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(vibration_data[:1024])
        ax.set_xlabel("é‡‡æ ·ç‚¹")
        ax.set_ylabel("æŒ¯å¹…")
        ax.set_title("æŒ¯åŠ¨ä¿¡å·æ³¢å½¢ (å‰1024ä¸ªç‚¹)")
        ax.grid(True)
        st.pyplot(fig)
        
        # è¯Šæ–­æŒ‰é’®
        if st.button("å¼€å§‹æ•…éšœè¯Šæ–­", type="primary", key="diagnose_btn"):
            if len(vibration_data) < 1024:
                st.error(f"æ•°æ®é•¿åº¦ä¸è¶³1024ç‚¹ï¼Œå½“å‰åªæœ‰{len(vibration_data)}ç‚¹")
            else:
                with st.spinner("æ­£åœ¨åˆ†ææ•°æ®..."):
                    # æ»‘åŠ¨çª—å£åˆ†å‰²
                    time_steps = 1024
                    overlap_ratio = 0.5
                    split_new_data = split_data_with_overlap(
                        vibration_data, time_steps, label=-1, overlap_ratio=overlap_ratio
                    )
                    
                    # æå–ç‰¹å¾éƒ¨åˆ†å¹¶è½¬æ¢ä¸ºå¼ é‡
                    input_new = split_new_data.iloc[:, 0:time_steps]
                    input_new_tensor = torch.tensor(input_new.values).float().to(device)

                    # æ¨¡å‹é¢„æµ‹
                    predictions = []
                    all_probabilities_list = []

                    with torch.no_grad():
                        for i in range(input_new_tensor.size(0)):
                            output = model(input_new_tensor[i].unsqueeze(0))
                            _, predicted = torch.max(output.data, 1)
                            predictions.append(predicted.item())
                            probabilities = torch.nn.functional.softmax(output, dim=1)
                            all_probabilities_list.append(probabilities.cpu().numpy()[0])

                    # è®¡ç®—æ•´ä½“é¢„æµ‹ç»“æœ
                    if predictions:
                        prediction_counter = Counter(predictions)
                        most_common = prediction_counter.most_common(1)
                        predicted_label = most_common[0][0] if most_common else 0
                        predicted_class = fault_type_mapping.get(predicted_label, f"æœªçŸ¥æ•…éšœ({predicted_label})")
                        
                        avg_probabilities = np.mean(all_probabilities_list, axis=0)
                        confidence_value = avg_probabilities[predicted_label]
                        
                        # å­˜å‚¨åˆ†æç»“æœ
                        analysis_results = {
                            "æœ€å°å€¼": f"{np.min(vibration_data):.4f}",
                            "æœ€å¤§å€¼": f"{np.max(vibration_data):.4f}",
                            "å¹³å‡å€¼": f"{np.mean(vibration_data):.4f}",
                            "æ ‡å‡†å·®": f"{np.std(vibration_data):.4f}",
                            "æ•°æ®ç‚¹æ•°": len(vibration_data),
                            "åˆ†ææ ·æœ¬æ•°": len(predictions)
                        }
                        
                        # å­˜å‚¨ç»“æœåˆ°session state
                        st.session_state.diagnosis_completed = True
                        st.session_state.predicted_class = predicted_class
                        st.session_state.confidence_value = confidence_value
                        st.session_state.analysis_results = analysis_results
                        st.session_state.avg_probabilities = avg_probabilities
                        st.session_state.show_report = False  # é‡ç½®æŠ¥å‘Šæ˜¾ç¤ºçŠ¶æ€

                        # ç«‹å³è®¾ç½®diagnosis_resultsï¼Œä»¥ä¾¿åœ¨å¯¹è¯ä¸­ä½¿ç”¨
                        st.session_state['diagnosis_results'] = {
                            'diagnosis_class': predicted_class,
                            'confidence_level': confidence_value,
                            'analysis_results': analysis_results,
                            'report': None  # æ­¤æ—¶è¿˜æ²¡æœ‰æŠ¥å‘Šï¼Œç”ŸæˆæŠ¥å‘Šåå†æ›´æ–°
                        }
                        
                        # æ·»åŠ ä¸€ç‚¹å»¶è¿Ÿ
                        time.sleep(1)
    
    except Exception as e:
        st.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {e}")

# æ˜¾ç¤ºè¯Šæ–­ç»“æœï¼ˆå¦‚æœå·²å®Œæˆè¯Šæ–­ï¼‰
if st.session_state.diagnosis_completed:
    st.markdown("---")
    st.subheader("ğŸ” è¯Šæ–­ç»“æœ")
    
    # æ·»åŠ æç¤ºä¿¡æ¯
    st.info("è¯Šæ–­å·²å®Œæˆï¼æ‚¨ç°åœ¨å¯ä»¥ä¸AIåŠ©æ‰‹å¯¹è¯ï¼Œè·å–åŸºäºè¯Šæ–­ç»“æœçš„è¯¦ç»†è§£é‡Šå’Œå»ºè®®ã€‚")
    
    # ä½¿ç”¨åˆ—å¸ƒå±€æ˜¾ç¤ºä¸»è¦ç»“æœ
    col1, col2 = st.columns(2)
    
    with col1:
        if st.session_state.predicted_class.startswith("æ­£å¸¸çŠ¶æ€"):
            st.success(f"**è¯Šæ–­ç»“æœ**: {st.session_state.predicted_class}")
        else:
            st.error(f"**è¯Šæ–­ç»“æœ**: {st.session_state.predicted_class}")
        st.info(f"**ç½®ä¿¡åº¦**: {st.session_state.confidence_value*100:.2f}%")
    
    with col2:
        # æ˜¾ç¤ºæ¦‚ç‡æœ€é«˜çš„å‰3ä¸ªæ•…éšœç±»å‹
        top3_indices = np.argsort(st.session_state.avg_probabilities)[-3:][::-1]
        st.write("**æ¦‚ç‡æœ€é«˜çš„æ•…éšœç±»å‹**:")
        for i, idx in enumerate(top3_indices):
            prob = st.session_state.avg_probabilities[idx] * 100
            fault_name = fault_type_mapping.get(idx, f"æœªçŸ¥æ•…éšœ{idx}")
            st.write(f"{i+1}. {fault_name}: {prob:.2f}%")
    
    # æ˜¾ç¤ºæ•…éšœæ¦‚ç‡åˆ†å¸ƒ
    st.subheader("æ•…éšœæ¦‚ç‡åˆ†å¸ƒ")
    prob_df = pd.DataFrame({
        "æ•…éšœç±»å‹": [fault_type_mapping.get(i, f"æœªçŸ¥{i}") for i in range(10)],
        "æ¦‚ç‡(%)": [p * 100 for p in st.session_state.avg_probabilities]
    })
    st.bar_chart(prob_df.set_index("æ•…éšœç±»å‹"))
    
    # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
    with st.expander("æŸ¥çœ‹è¯¦ç»†åˆ†æ"):
        st.write("**åŸå§‹æŒ¯åŠ¨ä¿¡å·ç»Ÿè®¡**:")
        for key, value in st.session_state.analysis_results.items():
            st.write(f"- {key}: {value}")
        
        st.write("**åˆ†æè¯´æ˜**:")
        if st.session_state.predicted_class.startswith("æ­£å¸¸çŠ¶æ€"):
            st.write("æŒ¯åŠ¨ä¿¡å·ç‰¹å¾æ˜¾ç¤ºç”µæœºå¤„äºæ­£å¸¸è¿è¡ŒçŠ¶æ€ï¼Œæ— æ˜æ˜¾æ•…éšœç‰¹å¾ã€‚")
        else:
            st.write(f"æ£€æµ‹åˆ°æ•…éšœç‰¹å¾ï¼Œæœ€å¯èƒ½çš„åŸå› æ˜¯: {st.session_state.predicted_class.split('(')[0]}")
            
            if "å†…åœˆ" in st.session_state.predicted_class:
                st.write("**å»ºè®®**: æ£€æŸ¥ç”µæœºè½´æ‰¿å†…åœˆæ˜¯å¦æœ‰ç£¨æŸã€è£‚çº¹æˆ–ç‚¹èš€ã€‚")
            elif "å¤–åœˆ" in st.session_state.predicted_class:
                st.write("**å»ºè®®**: æ£€æŸ¥ç”µæœºè½´æ‰¿å¤–åœˆæ˜¯å¦æœ‰ç£¨æŸã€è£‚çº¹æˆ–ç‚¹èš€ã€‚")
            elif "æ»šåŠ¨ä½“" in st.session_state.predicted_class:
                st.write("**å»ºè®®**: æ£€æŸ¥ç”µæœºè½´æ‰¿æ»šåŠ¨ä½“æ˜¯å¦æœ‰ç£¨æŸã€è£‚çº¹æˆ–ç¼ºå¤±ã€‚")
            
            st.write("å»ºè®®å°½å¿«å®‰æ’ä¸“ä¸šäººå‘˜è¿›è¡Œæ£€æŸ¥å’Œç»´æŠ¤ã€‚")
    
    # ç”Ÿæˆè¯Šæ–­æŠ¥å‘ŠæŒ‰é’®
    if st.button("ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š", key="generate_report_btn"):
        st.session_state.show_report = True
    
    # # æ˜¾ç¤ºè¯Šæ–­æŠ¥å‘Šï¼ˆå¦‚æœç”¨æˆ·ç‚¹å‡»äº†ç”ŸæˆæŠ¥å‘ŠæŒ‰é’®ï¼‰
    # if st.session_state.show_report:
    #     generate_diagnostic_report(
    #         st.session_state.predicted_class, 
    #         st.session_state.confidence_value,
    #         st.session_state.analysis_results
    #     )

    # æ˜¾ç¤ºè¯Šæ–­æŠ¥å‘Šï¼ˆå¦‚æœç”¨æˆ·ç‚¹å‡»äº†ç”ŸæˆæŠ¥å‘ŠæŒ‰é’®ï¼‰
if st.session_state.show_report:
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç”Ÿæˆçš„æŠ¥å‘Šæ•°æ®
    if (st.session_state['diagnosis_results'] and 
        st.session_state['diagnosis_results'].get('pdf_data') and 
        st.session_state['diagnosis_results'].get('docx_data')):
        
        # ç›´æ¥æ˜¾ç¤ºä¸‹è½½æŒ‰é’®ï¼Œä¸é‡æ–°ç”ŸæˆæŠ¥å‘Š
        st.subheader("ğŸ“Š AIå¤§æ¨¡å‹è§£é‡Šä¸å»ºè®®")
        st.write(st.session_state['diagnosis_results']['report'])
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                label="ä¸‹è½½è¯Šæ–­æŠ¥å‘Š (PDF)",
                data=st.session_state['diagnosis_results']['pdf_data'],
                file_name=f"è®¾å¤‡è¯Šæ–­æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                mime="application/pdf"
            )
        with col2:
            st.download_button(
                label="ä¸‹è½½è¯Šæ–­æŠ¥å‘Š (DOCX)",
                data=st.session_state['diagnosis_results']['docx_data'],
                file_name=f"è®¾å¤‡è¯Šæ–­æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
    else:
        # å¦‚æœæ²¡æœ‰ç”Ÿæˆçš„æŠ¥å‘Šæ•°æ®ï¼Œè°ƒç”¨å‡½æ•°ç”Ÿæˆ
        generate_diagnostic_report(
            st.session_state.predicted_class, 
            st.session_state.confidence_value,
            st.session_state.analysis_results
        )

    # æ˜¾ç¤ºè¯Šæ–­çŠ¶æ€
    if 'diagnosis_results' in st.session_state and st.session_state.diagnosis_results:
        st.success("âœ… å·²æœ‰è¯Šæ–­ç»“æœï¼ŒAIåŠ©æ‰‹å°†åŸºäºæ­¤ç»“æœå›ç­”æ‚¨çš„é—®é¢˜")
    else:
        st.info("â„¹ï¸ æš‚æ— è¯Šæ–­ç»“æœï¼ŒAIåŠ©æ‰‹å°†æä¾›ä¸€èˆ¬æ€§å»ºè®®ã€‚è¯·å…ˆç”Ÿæˆè¯Šæ–­æŠ¥å‘Šã€‚")

    # æ¸…é™¤æŒ‰é’®
    if st.button("æ¸…é™¤è¯Šæ–­ç»“æœå’Œå¯¹è¯å†å²"):
        if 'diagnosis_results' in st.session_state:
            del st.session_state['diagnosis_results']
        st.session_state["messages"] = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œå¸®åŠ©ç”¨æˆ·åˆ†æä¿¡å·å¹¶ç»™å‡ºè¯Šæ–­å»ºè®®ã€‚"}
        ]
        st.success("å·²æ¸…é™¤è¯Šæ–­ç»“æœå’Œå¯¹è¯å†å²")
        st.rerun()

    # èŠå¤©æ¡†æç¤º
    if st.session_state.vector_index is not None:
        st.info("ğŸ“š å½“å‰å·²åŠ è½½æ•…éšœæ‰‹å†Œå‘é‡çŸ¥è¯†åº“ï¼ŒAIåŠ©æ‰‹å°†æ™ºèƒ½æ£€ç´¢ç›¸å…³å†…å®¹")
    else:
        st.warning("ğŸ“š æœªåŠ è½½æ•…éšœæ‰‹å†Œï¼ŒAIåŠ©æ‰‹å°†åŸºäºé€šç”¨çŸ¥è¯†å›ç­”ã€‚æ‚¨å¯ä»¥åœ¨ä¾§è¾¹æ ä¸Šä¼ æ•…éšœæ‰‹å†Œ")

    # -----------------------------
    # èŠå¤©æ¡† - æ”¾åœ¨è¯Šæ–­ç»“æœä¸‹æ–¹
    # -----------------------------
    st.markdown("---")
    st.subheader("ğŸ’¬ ç”µæœºè¯Šæ–­åŠ©æ‰‹å¯¹è¯")

    # æ˜¾ç¤ºå†å²å¯¹è¯
    for msg in st.session_state["messages"]:
        if msg["role"] == "user":
            st.chat_message("user").write(msg["content"])
        elif msg["role"] == "assistant":
            st.chat_message("assistant").write(msg["content"])

    # è¾“å…¥æ¡†
    if prompt := st.chat_input("è¾“å…¥é—®é¢˜ï¼Œä¾‹å¦‚ï¼šè¯·åˆ†æå½“å‰ç”µæœºä¿¡å·æ˜¯å¦å¼‚å¸¸"):
        # ä¿å­˜ç”¨æˆ·è¾“å…¥
        st.session_state["messages"].append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        # -----------------------------
        # è°ƒç”¨AI APIæ¥å£
        # -----------------------------
        try:
            reply = call_ai_api(
                messages=st.session_state["messages"],
                max_tokens=2000,
                temperature=0.7
            )
            
            if reply is None:
                reply = "âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯ä¿¡æ¯"
                
        except Exception as e:
            reply = f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

        # ä¿å­˜åŠ©æ‰‹å›å¤ 
        st.session_state["messages"].append({"role": "assistant", "content": reply})
        st.chat_message("assistant").write(reply)
# åªåœ¨æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ä¸”æ²¡æœ‰è¯Šæ–­ç»“æœæ—¶æ˜¾ç¤ºæç¤º
if uploaded_file is None and not st.session_state.diagnosis_completed:
    st.info("è¯·ä¸Šä¼ CSVæ ¼å¼çš„æŒ¯åŠ¨æ•°æ®æ–‡ä»¶å¼€å§‹åˆ†æ")

# æ·»åŠ é¡µè„š
st.markdown("---")

st.markdown("ç”µæœºæ•…éšœè¯Šæ–­ç³»ç»Ÿ ğŸ”§ 2025 | åŸºäºPyTorchå’ŒStreamlitå¼€å‘")
